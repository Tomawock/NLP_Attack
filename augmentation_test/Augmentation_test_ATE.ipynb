{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Augmentation_test_ATE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNfehtkpAwQjHpbdqJYe8Xs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tomawock/NLP_Attack/blob/main/Augmentation_test_ATE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeVVV3Z6Cn5L"
      },
      "source": [
        "!pip install textattack"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KnCAl2ZCs7R"
      },
      "source": [
        "import random\n",
        "import timeit"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lK8QmX4C1Ol"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset = pd.read_json('/content/drive/Shareddrives/Deep Learning/datasets/ATE_ABSITA/ATE_ABSITA_training_set/ate_absita_training.ndjson'\n",
        "                       , lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3brDTf7iHBBH"
      },
      "source": [
        "dataset.drop(columns=['id_sentence'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "SsoiD7PXEh2p",
        "outputId": "4d08a49e-3726-4f1d-b0a8-f68a981ebf8a"
      },
      "source": [
        "print(f'Contains {len(dataset)} sentences')\n",
        "dataset.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Contains 3054 sentences\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>score</th>\n",
              "      <th>polarities</th>\n",
              "      <th>aspects_position</th>\n",
              "      <th>aspects</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ottimo prodotto di marca, la qualità é veramen...</td>\n",
              "      <td>5</td>\n",
              "      <td>[[0, 0], [0, 1], [1, 0]]</td>\n",
              "      <td>[[120, 142], [71, 79], [29, 36]]</td>\n",
              "      <td>[provvisto di una tasca, capiente, qualità]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ottimo rasoio dal semplice utilizzo. Rade molt...</td>\n",
              "      <td>5</td>\n",
              "      <td>[[1, 0], [1, 0], [1, 0], [1, 0]]</td>\n",
              "      <td>[[18, 26], [37, 41], [79, 86], [99, 105]]</td>\n",
              "      <td>[semplice, Rade, Pratico, pulire]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Un quarto delle dimensioni dello Show original...</td>\n",
              "      <td>5</td>\n",
              "      <td>[[1, 0], [1, 0], [1, 0], [0, 0]]</td>\n",
              "      <td>[[118, 132], [51, 62], [65, 70], [16, 26]]</td>\n",
              "      <td>[modalità notte, prestazioni, suono, dimensioni]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Il prodotto si presenta esattamente come in fo...</td>\n",
              "      <td>5</td>\n",
              "      <td>[[1, 0], [1, 0], [1, 0]]</td>\n",
              "      <td>[[147, 158], [132, 140], [24, 48]]</td>\n",
              "      <td>[vestibilità, capienza, esattamente come in foto]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Superlativa, velocità in scrittura superiore a...</td>\n",
              "      <td>5</td>\n",
              "      <td>[[1, 0]]</td>\n",
              "      <td>[[13, 21]]</td>\n",
              "      <td>[velocità]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...                                            aspects\n",
              "0  Ottimo prodotto di marca, la qualità é veramen...  ...        [provvisto di una tasca, capiente, qualità]\n",
              "1  Ottimo rasoio dal semplice utilizzo. Rade molt...  ...                  [semplice, Rade, Pratico, pulire]\n",
              "2  Un quarto delle dimensioni dello Show original...  ...   [modalità notte, prestazioni, suono, dimensioni]\n",
              "3  Il prodotto si presenta esattamente come in fo...  ...  [vestibilità, capienza, esattamente come in foto]\n",
              "4  Superlativa, velocità in scrittura superiore a...  ...                                         [velocità]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z6YhkUwHg5z"
      },
      "source": [
        "###Sono presenti solo 20 duplicati. Si decide di non rimuoverli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W54DNrzJGr9f",
        "outputId": "d3f7b7a3-4355-466c-ee0d-20bd1e3eae13"
      },
      "source": [
        "len(dataset.groupby('sentence'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3034"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SECu86H-FQqT"
      },
      "source": [
        "aug = dataset.copy()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsE7HlbAk0Bt"
      },
      "source": [
        "##Augmentation Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFA8DbunkiTk"
      },
      "source": [
        "###WordNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHeTXUiaLO3N",
        "outputId": "ca00c678-625b-4af0-898f-c322ee7537c1"
      },
      "source": [
        "from textattack.transformations import WordSwapWordNet\n",
        "\n",
        "from textattack.constraints.pre_transformation import RepeatModification\n",
        "from textattack.constraints.pre_transformation import StopwordModification\n",
        "\n",
        "from textattack.augmentation import Augmenter\n",
        "random.seed(33)\n",
        "\n",
        "\n",
        "transformation = WordSwapWordNet(language='ita')\n",
        "constraints = [RepeatModification(), StopwordModification()]\n",
        "\n",
        "my_items = []\n",
        "num_tras_to_add = 2\n",
        "\n",
        "start_time = timeit.default_timer()\n",
        "augmenter = Augmenter(transformation=transformation,\n",
        "                      constraints=constraints,\n",
        "                      pct_words_to_swap=1,\n",
        "                      transformations_per_example=num_tras_to_add)\n",
        "\n",
        "for row in aug.head(10).itertuples():\n",
        "  result=augmenter.augment(row.sentence)\n",
        "  my_items.append({'sentence': row.sentence,\n",
        "                   'score': row.score,\n",
        "                   'polarities': row.polarities,\n",
        "                   'aspects_position': row.aspects_position,\n",
        "                   'aspects': row.aspects})\n",
        "  for i in range(num_tras_to_add):\n",
        "    ### QUI MODIFICHE\n",
        "    my_items.append({'sentence': result[i],\n",
        "                     'score': row.score,\n",
        "                     'polarities': row.polarities,\n",
        "                     'aspects_position': row.aspects_position,\n",
        "                     'aspects': row.aspects})\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(f'TIME OF EXECUTION={elapsed}')\n",
        "augmeted_new = pd.DataFrame(data=my_items, columns=aug.columns)\n",
        "print(f\"COLONNE:{augmeted_new.columns}\")\n",
        "print(f\"DIMENSIONE:{augmeted_new.shape}\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TIME OF EXECUTION=0.5047117140002229\n",
            "COLONNE:Index(['sentence', 'score', 'polarities', 'aspects_position', 'aspects'], dtype='object')\n",
            "DIMENSIONE:(6, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFoZuemqlBen"
      },
      "source": [
        "augmeted_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxJiwtawk3ib"
      },
      "source": [
        "###WordEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c35MweMsY2uA"
      },
      "source": [
        "from textattack.shared import WordEmbedding\n",
        "\n",
        "class WordEmbeddingIta(WordEmbedding):\n",
        "    def counterfitted_GLOVE_embedding():\n",
        "      \"\"\"Returns a prebuilt counter-fitted GLOVE word embedding proposed by\n",
        "      \"Counter-fitting Word Vectors to Linguistic Constraints\" (Mrkšić et\n",
        "      al., 2016)\"\"\"\n",
        "      if (\n",
        "          \"textattack_counterfitted_GLOVE_embedding\" in utils.GLOBAL_OBJECTS\n",
        "          and isinstance(\n",
        "              utils.GLOBAL_OBJECTS[\"textattack_counterfitted_GLOVE_embedding\"],\n",
        "              WordEmbedding,\n",
        "          )\n",
        "      ):\n",
        "          # avoid recreating same embedding (same memory) and instead share across different components\n",
        "          return utils.GLOBAL_OBJECTS[\"textattack_counterfitted_GLOVE_embedding\"]\n",
        "\n",
        "      word_embeddings_folder = \"paragramcf\"   # NOSTRO PATH\n",
        "      word_embeddings_file = \"paragram.npy\"\n",
        "      word_list_file = \"wordlist.pickle\"\n",
        "      mse_dist_file = \"mse_dist.p\"\n",
        "      cos_sim_file = \"cos_sim.p\"\n",
        "      nn_matrix_file = \"nn.npy\"\n",
        "\n",
        "      # Download embeddings if they're not cached.\n",
        "      word_embeddings_folder = os.path.join(WordEmbedding.PATH,\n",
        "                                            word_embeddings_folder)\n",
        "\n",
        "      word_embeddings_folder = utils.download_if_needed(word_embeddings_folder)\n",
        "      # Concatenate folder names to create full path to files.\n",
        "      word_embeddings_file = os.path.join(word_embeddings_folder,\n",
        "                                          word_embeddings_file)\n",
        "\n",
        "      word_list_file = os.path.join(word_embeddings_folder, word_list_file)\n",
        "      mse_dist_file = os.path.join(word_embeddings_folder, mse_dist_file)\n",
        "      cos_sim_file = os.path.join(word_embeddings_folder, cos_sim_file)\n",
        "      nn_matrix_file = os.path.join(word_embeddings_folder, nn_matrix_file)\n",
        "\n",
        "      # loading the files\n",
        "      embedding_matrix = np.load(word_embeddings_file)\n",
        "      word2index = np.load(word_list_file, allow_pickle=True)\n",
        "      index2word = {}\n",
        "      for word, index in word2index.items():\n",
        "          index2word[index] = word\n",
        "      nn_matrix = np.load(nn_matrix_file)\n",
        "\n",
        "      embedding = WordEmbedding(embedding_matrix, word2index, index2word, nn_matrix)\n",
        "\n",
        "      with open(mse_dist_file, \"rb\") as f:\n",
        "          mse_dist_mat = pickle.load(f)\n",
        "      with open(cos_sim_file, \"rb\") as f:\n",
        "          cos_sim_mat = pickle.load(f)\n",
        "\n",
        "      embedding._mse_dist_mat = mse_dist_mat\n",
        "      embedding._cos_sim_mat = cos_sim_mat\n",
        "\n",
        "      utils.GLOBAL_OBJECTS[\"textattack_counterfitted_GLOVE_embedding\"] = embedding\n",
        "\n",
        "      return embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRL_RRJHYYCy"
      },
      "source": [
        "from textattack.augmentation import EmbeddingAugmenter\n",
        "\n",
        "num_tras_to_add = 2\n",
        "augmenter = EmbeddingAugmenter(pct_words_to_swap=1,\n",
        "                               transformations_per_example=num_tras_to_add,\n",
        "                               embedding=WordEmbedding.counterfitted_GLOVE_embedding())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGZ5VGiolJ1O"
      },
      "source": [
        "augmeted_new"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
