{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ate.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tomawock/NLP_Attack/blob/main/augmentation/ate/ate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcUBYjgYWgWk"
      },
      "source": [
        "!pip install textattack"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6saeB2sIW9Bo",
        "outputId": "b71fd381-7a21-40e1-ea22-d8c53591454f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrnnXLrMXJds"
      },
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30iR2nLYXIH1"
      },
      "source": [
        "ate_folder = '/content/drive/Shareddrives/Deep Learning/datasets/ATE_ABSITA/'\n",
        "testset = pd.read_json(f'{ate_folder}/ATE_ABSITA_test_set/ate_absita_gold.ndjson',\n",
        "                       lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVIff8UxYFQC"
      },
      "source": [
        "testset.drop(columns=['id_sentence'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LELeq0Pid9tM"
      },
      "source": [
        "#Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBpwIkqejLOE"
      },
      "source": [
        "from textattack.constraints.pre_transformation import RepeatModification\n",
        "from textattack.constraints.pre_transformation import StopwordModification\n",
        "\n",
        "constraints = [RepeatModification(), StopwordModification()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIivxBg1eBGD"
      },
      "source": [
        "##WordNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY07caFaYOrj"
      },
      "source": [
        "from textattack.transformations import WordSwapWordNet\n",
        "from textattack.augmentation import Augmenter\n",
        "\n",
        "random.seed(33)\n",
        "\n",
        "transformation = WordSwapWordNet(language='ita')\n",
        "\n",
        "my_items = []\n",
        "num_tras_to_add = 1\n",
        "\n",
        "augmenter = Augmenter(transformation=transformation,\n",
        "                      constraints=constraints,\n",
        "                      pct_words_to_swap=0.5,  # Oppure 0.05\n",
        "                      transformations_per_example=num_tras_to_add)\n",
        "\n",
        "for row in testset.itertuples():\n",
        "  result=augmenter.augment(row.sentence)\n",
        "  for i in range(num_tras_to_add):\n",
        "    ### QUI MODIFICHE\n",
        "    my_items.append({'sentence': result[i],\n",
        "                     'score': row.score,\n",
        "                     'polarities': row.polarities,\n",
        "                     'aspects_position': row.aspects_position,\n",
        "                     'aspects': row.aspects})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLoStDMrY5df"
      },
      "source": [
        "pd.DataFrame(data=my_items, columns=testset.columns).to_csv(f'{ate_folder}/ATE_ABSITA_test_set/sinonimi.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxgqafCoeG2k"
      },
      "source": [
        "\n",
        "##WordEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t45lR-OieMUC"
      },
      "source": [
        "with open(\"/content/drive/Shareddrives/Deep Learning/datasets/ATE_ABSITA/index2word.pkl\", 'rb') as output:\n",
        "  i2w = pickle.load(output)\n",
        "with open(\"/content/drive/Shareddrives/Deep Learning/datasets/ATE_ABSITA/word2index.pkl\", 'rb') as output:\n",
        "  w2i = pickle.load(output)\n",
        "with open(\"/content/drive/Shareddrives/Deep Learning/datasets/ATE_ABSITA/nn.pkl\", 'rb') as output:\n",
        "  nn_matrix = pickle.load(output)\n",
        "with open(\"/content/drive/Shareddrives/Deep Learning/datasets/ATE_ABSITA/embedding_matrix.pkl\", 'rb') as output:\n",
        "  embedding_matrix = pickle.load(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxsfryEMeitA"
      },
      "source": [
        "import textattack\n",
        "\n",
        "embedding = textattack.shared.WordEmbedding(embedding_matrix, w2i, i2w, nn_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WURNfQQ4g-D8"
      },
      "source": [
        "from textattack.transformations import WordSwapEmbedding\n",
        "from textattack.augmentation import Augmenter\n",
        "random.seed(33)\n",
        "\n",
        "transformation = WordSwapEmbedding(20, embedding)\n",
        "\n",
        "my_items = []\n",
        "num_tras_to_add = 1\n",
        "\n",
        "augmenter = Augmenter(transformation=transformation,\n",
        "                      constraints=constraints,\n",
        "                      pct_words_to_swap=0.5,  # Oppure 0.05\n",
        "                      transformations_per_example=num_tras_to_add)\n",
        "\n",
        "for row in testset.itertuples():\n",
        "  result=augmenter.augment(row.sentence)\n",
        "  for i in range(num_tras_to_add):\n",
        "    ### QUI MODIFICHE\n",
        "    my_items.append({'sentence': result[i],\n",
        "                     'score': row.score,\n",
        "                     'polarities': row.polarities,\n",
        "                     'aspects_position': row.aspects_position,\n",
        "                     'aspects': row.aspects})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNCE-jeEhVXv"
      },
      "source": [
        "pd.DataFrame(data=my_items, columns=testset.columns).to_csv(f'{ate_folder}/ATE_ABSITA_test_set/embedding.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}