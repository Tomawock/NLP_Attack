{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GMB.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tomawock/NLP_Attack/blob/main/models/studio_2/GMB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POK6j5oiTzzi"
      },
      "source": [
        "Parte nuova testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4D6g8HCVCnq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f900af-41ce-4005-badd-d17063b82d9d"
      },
      "source": [
        "!pip install h5py\n",
        "!pip install transformers"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py) (1.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLcY-sL2EW4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635d8aa2-2189-48fb-98e9-883662b34ab5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EsqztNaYcQE"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from sklearn import preprocessing\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "import numpy as np"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCjC_sLPVsC7"
      },
      "source": [
        "data_original = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_dataset.tsv\"\n",
        "max_len = 384"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rsWWOi_YPtO"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False, reduction=tf.keras.losses.Reduction.SUM\n",
        ")\n",
        "\n",
        "# Ignoring loss that is calculated due to padded targets\n",
        "def masked_ce_loss(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 17))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "def create_model(num_tags):\n",
        "    # BERT Base model\n",
        "    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # NER Model\n",
        "\n",
        "    # Instantiate Keras tensors\n",
        "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    # TFBertmodel\n",
        "    embedding = encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
        "    # Add drop out layer\n",
        "    embedding = layers.Dropout(0.5)(embedding)\n",
        "    # Add softmax layer for classifying\n",
        "    tag_logits = layers.Dense(num_tags+1, activation='softmax')(embedding)\n",
        "    \n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask],\n",
        "        outputs=[tag_logits],\n",
        "    )\n",
        "    optimizer = keras.optimizers.Adam(lr=3e-5)\n",
        "    model.compile(optimizer=optimizer, loss=masked_ce_loss, metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl2kaHNtcO-F"
      },
      "source": [
        "#Reference - https://keras.io/examples/nlp/text_extraction_with_bert/\n",
        "# Save the slow pretrained tokenizer\n",
        "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "save_path = \"bert_base_uncased/\"\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "slow_tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Load the fast tokenizer from saved file\n",
        "tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQxXGab4Vg5U"
      },
      "source": [
        "parz = pd.read_csv(data_original,sep='\\t', encoding=\"latin-1\", index_col=[0])\n",
        "\n",
        "num_tags = parz[\"Tag\"].nunique()\n",
        "\n",
        "new_model = create_model(num_tags)\n",
        "    \n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mssGj3zjI2hh"
      },
      "source": [
        "###Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKYEp_xsaQJ4"
      },
      "source": [
        "# Get the sentences\n",
        "def process_csv(data_path):\n",
        "    df = pd.read_csv(data_path, encoding=\"latin-1\", na_filter = False)\n",
        "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
        "    enc_tag = preprocessing.LabelEncoder()\n",
        "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
        "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "    return sentences, tag, enc_tag\n",
        "\n",
        "# Get the sentences\n",
        "def process_tsv(data_path):\n",
        "    df =  pd.read_csv(data_path,sep='\\t', encoding=\"latin-1\", index_col=[0])\n",
        "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
        "    enc_tag = preprocessing.LabelEncoder()\n",
        "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
        "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "    return sentences, tag, enc_tag\n",
        "\n",
        "# Prepare the inputs for feeding into model\n",
        "def create_inputs_targets(data):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"tags\": []\n",
        "    }\n",
        "    _,ext=os.path.splitext(data)\n",
        "    if (ext== '.tsv'):\n",
        "      sentences, tags, tag_encoder = process_tsv(data)\n",
        "    else:\n",
        "      sentences, tags, tag_encoder = process_csv(data)\n",
        "    \n",
        "    for sentence, tag in zip(sentences, tags):\n",
        "        \n",
        "        input_ids = []\n",
        "        target_tags = []\n",
        "        for idx, word in enumerate(sentence):\n",
        "            ids = tokenizer.encode(word, add_special_tokens=False)\n",
        "            input_ids.extend(ids.ids)\n",
        "            num_tokens = len(ids)\n",
        "            target_tags.extend([tag[idx]] * num_tokens)\n",
        "                    \n",
        "        \n",
        "        # Pad truncate\n",
        "        input_ids = input_ids[:max_len - 2]\n",
        "        target_tags = target_tags[:max_len - 2]\n",
        "\n",
        "        # Add [CLS] and [SEP]\n",
        "        input_ids = [101] + input_ids + [102]\n",
        "        target_tags = [16] + target_tags + [16]\n",
        "        # token_type_ids does not matter as the task has only one sentence\n",
        "        token_type_ids = [0] * len(input_ids)\n",
        "        # Adding attention mask for non-padded input\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        \n",
        "        # Making all the input enbedding same lenting by padding\n",
        "        padding_len = max_len - len(input_ids)\n",
        "        input_ids = input_ids + ([0] * padding_len)\n",
        "        attention_mask = attention_mask + ([0] * padding_len)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "        target_tags = target_tags + ([17] * padding_len)\n",
        "        \n",
        "        dataset_dict[\"input_ids\"].append(input_ids)\n",
        "        dataset_dict[\"token_type_ids\"].append(token_type_ids)\n",
        "        dataset_dict[\"attention_mask\"].append(attention_mask)\n",
        "        dataset_dict[\"tags\"].append(target_tags)\n",
        "        assert len(target_tags) == max_len, f'{len(input_ids)}, {len(target_tags)}'\n",
        "                \n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    # Creating array of input embeddings\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"token_type_ids\"],\n",
        "        dataset_dict[\"attention_mask\"],\n",
        "    ]\n",
        "    y = dataset_dict[\"tags\"]\n",
        "    return x, y, tag_encoder"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y36IkdBRPaev"
      },
      "source": [
        "data_original_embedding_train = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_original_embeding_train.csv\""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGeGXmfcI1jU"
      },
      "source": [
        "x_train, y_train, tag_encoder = create_inputs_targets(data_original_embedding_train)\n",
        "\n",
        "bs = 16\n",
        "\n",
        "new_model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=1,\n",
        "    verbose=1,\n",
        "    batch_size=bs,\n",
        "    validation_split=0.1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF-oLRMgV65C"
      },
      "source": [
        "new_model.save_weights(\"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_w_studio_2_model.h5\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u1o8pPySLR2"
      },
      "source": [
        "###Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-J7Xft_VWVT"
      },
      "source": [
        "new_model.load_weights(\"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_w_studio_2_model.h5\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkQHJBaBdfFU"
      },
      "source": [
        "datapath_original_embedding_test = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_original_embeding_test.csv\"\n",
        "datapath_original_test = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_original_test.csv\"\n",
        "datapath_sinonimi_test = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_sinonimi_test.csv\"\n",
        "datapath_embedding_test = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_embedding_test.csv\""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds4YMx3teKZD"
      },
      "source": [
        "Original + embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72Mk0Zn4SkOX",
        "outputId": "54a0b37e-c519-4597-c1af-3323c6b4c62e"
      },
      "source": [
        "#crea i dati per essere elborati dal modello\n",
        "x_test, y_test, tag_encoder = create_inputs_targets(datapath_original_embedding_test)\n",
        "pred_test = new_model.evaluate(x_test,y_test,batch_size=16)\n",
        "#definisce il dizioanrio dell'associazione classi valore numerico"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "113/113 [==============================] - 53s 466ms/step - loss: 220.7127 - accuracy: 0.9371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cQdMq8Re3Qb"
      },
      "source": [
        "Original\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFc4tfOHe5RE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e170d350-c212-4b64-e664-ad41f2a777d5"
      },
      "source": [
        "#crea i dati per essere elborati dal modello\n",
        "x_test, y_test, tag_encoder = create_inputs_targets(datapath_original_test)\n",
        "pred_test = new_model.evaluate(x_test,y_test,batch_size=16)\n",
        "#definisce il dizioanrio dell'associazione classi valore numerico"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 26s 447ms/step - loss: 199.9259 - accuracy: 0.9409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-DCDEnySdno"
      },
      "source": [
        "Sinonimi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVfSNN8vV6-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c4954f-868f-429d-8f6b-daf6209bc33e"
      },
      "source": [
        "#crea i dati per essere elborati dal modello\n",
        "x_test, y_test, tag_encoder = create_inputs_targets(datapath_sinonimi_test)\n",
        "pred_test = new_model.evaluate(x_test,y_test,batch_size=16)\n",
        "#definisce il dizioanrio dell'associazione classi valore numerico"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 26s 457ms/step - loss: 260.7253 - accuracy: 0.9290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4HTD1kTSf89"
      },
      "source": [
        "Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT_yp3H2QTpf",
        "outputId": "9208e4ca-7c84-4569-c514-d71c0eb9014f"
      },
      "source": [
        "#crea i dati per essere elborati dal modello\n",
        "x_test, y_test, tag_encoder = create_inputs_targets(datapath_embedding_test)\n",
        "pred_test = new_model.evaluate(x_test,y_test,batch_size=16)\n",
        "#definisce il dizioanrio dell'associazione classi valore numerico"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 26s 463ms/step - loss: 241.5652 - accuracy: 0.9333\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}