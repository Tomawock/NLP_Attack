{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "nearby-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "healthy-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fourth-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "greater-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(path+'ATE_ABSITA_training_set/ate_absita_training.ndjson',\n",
    "                     lines=True)\n",
    "\n",
    "dev = pd.read_json(path+'ATE_ABSITA_dev_set/ate_absita_dev.ndjson',\n",
    "                   lines=True)\n",
    "\n",
    "test = pd.read_json(path+'ATE_ABSITA_test_set/ate_absita_gold.ndjson',\n",
    "                    lines=True)\n",
    "\n",
    "\n",
    "data_sinonimi = pd.read_csv(path+\"ATE_ABSITA_test_set/sinonimi.csv\")\n",
    "data_embedding = pd.read_csv(path+\"ATE_ABSITA_test_set/embedding.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "complex-future",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contains 3054 sentences\n",
      "Contains 109 sentences\n",
      "Contains 1200 sentences\n",
      "Contains 1200 sentences\n",
      "Contains 1200 sentences\n"
     ]
    }
   ],
   "source": [
    "train.drop(columns=['id_sentence','polarities','aspects_position','aspects'], inplace=True)\n",
    "dev.drop(columns=['id_sentence','polarities','aspects_position','aspects'], inplace=True)\n",
    "test.drop(columns=['id_sentence','polarities','aspects_position','aspects'], inplace=True)\n",
    "data_sinonimi.drop(columns=['polarities','aspects_position','aspects'], inplace=True)\n",
    "data_embedding.drop(columns=['polarities','aspects_position','aspects'], inplace=True)\n",
    "\n",
    "print(f'Contains {len(train)} sentences')\n",
    "print(f'Contains {len(dev)} sentences')\n",
    "print(f'Contains {len(test)} sentences')\n",
    "print(f'Contains {len(data_sinonimi)} sentences')\n",
    "print(f'Contains {len(data_embedding)} sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "according-revelation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN::\n",
      "pos    2150\n",
      "neg     904\n",
      "Name: review_type, dtype: int64\n",
      "DEV::\n",
      "pos    86\n",
      "neg    23\n",
      "Name: review_type, dtype: int64\n",
      "TEST::\n",
      "pos    857\n",
      "neg    343\n",
      "Name: review_type, dtype: int64\n",
      "SINONIMI::\n",
      "pos    857\n",
      "neg    343\n",
      "Name: review_type, dtype: int64\n",
      "EMBEDDING::\n",
      "pos    857\n",
      "neg    343\n",
      "Name: review_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train[\"review_type\"] = train[\"score\"].apply(lambda x: \"neg\" if x < 5 else \"pos\")\n",
    "dev[\"review_type\"] = dev[\"score\"].apply(lambda x: \"neg\" if x < 5 else \"pos\")\n",
    "test[\"review_type\"] = test[\"score\"].apply(lambda x: \"neg\" if x < 5 else \"pos\")\n",
    "data_sinonimi[\"review_type\"] = data_sinonimi[\"score\"].apply(lambda x: \"neg\" if x < 5 else \"pos\")\n",
    "data_embedding[\"review_type\"] = data_embedding[\"score\"].apply(lambda x: \"neg\" if x < 5 else \"pos\")\n",
    "\n",
    "print(f'TRAIN::\\n{train.review_type.value_counts()}')\n",
    "print(f'DEV::\\n{dev.review_type.value_counts()}')\n",
    "print(f'TEST::\\n{test.review_type.value_counts()}')\n",
    "print(f'SINONIMI::\\n{data_sinonimi.review_type.value_counts()}')\n",
    "print(f'EMBEDDING::\\n{data_embedding.review_type.value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "parental-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['score'], inplace=True)\n",
    "dev.drop(columns=['score'], inplace=True)\n",
    "test.drop(columns=['score'], inplace=True)\n",
    "data_sinonimi.drop(columns=['score'], inplace=True)\n",
    "data_embedding.drop(columns=['score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "precious-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_text_to_word_sequence(sentence):\n",
    "  return keras.preprocessing.text.text_to_word_sequence(sentence,\n",
    "                                                        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`\\'{|}~\\t\\n',\n",
    "                                                        lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-washington",
   "metadata": {},
   "source": [
    "# OneHotEncode delle frasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "light-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_train = OneHotEncoder(sparse=False).fit_transform(\n",
    "        train.review_type.to_numpy().reshape(-1, 1))\n",
    "\n",
    "sentences = [my_text_to_word_sequence(sentence) for sentence in train['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fewer-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_dev = OneHotEncoder(sparse=False).fit_transform(\n",
    "        dev.review_type.to_numpy().reshape(-1, 1))\n",
    "\n",
    "sentences_dev = [my_text_to_word_sequence(sentence) for sentence in dev['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pretty-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_test = OneHotEncoder(sparse=False).fit_transform(\n",
    "        test.review_type.to_numpy().reshape(-1, 1))\n",
    "\n",
    "sentences_test = [my_text_to_word_sequence(sentence) for sentence in test['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "activated-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_sin = OneHotEncoder(sparse=False).fit_transform(\n",
    "        data_sinonimi.review_type.to_numpy().reshape(-1, 1))\n",
    "\n",
    "sentences_sin = [my_text_to_word_sequence(sentence) for sentence in data_sinonimi['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "radical-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_emb = OneHotEncoder(sparse=False).fit_transform(\n",
    "        data_embedding.review_type.to_numpy().reshape(-1, 1))\n",
    "\n",
    "sentences_emb = [my_text_to_word_sequence(sentence) for sentence in data_embedding['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "illegal-phone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il massimo è 85\n"
     ]
    }
   ],
   "source": [
    "max_index, max = (-1, -1)\n",
    "for i, sentence in enumerate(sentences):\n",
    "  max_index, max = (i, len(sentence)) if len(sentence) > max else (max_index, max)\n",
    "for i, sentence in enumerate(sentences_dev):\n",
    "  max_index, max = (i, len(sentence)) if len(sentence) > max else (max_index, max)\n",
    "for i, sentence in enumerate(sentences_test):\n",
    "  max_index, max = (i, len(sentence)) if len(sentence) > max else (max_index, max)\n",
    "for i, sentence in enumerate(sentences_sin):\n",
    "  max_index, max = (i, len(sentence)) if len(sentence) > max else (max_index, max)\n",
    "for i, sentence in enumerate(sentences_emb):\n",
    "  max_index, max = (i, len(sentence)) if len(sentence) > max else (max_index, max)\n",
    "\n",
    "\n",
    "print(f'Il massimo è {max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-convertible",
   "metadata": {},
   "source": [
    "# Embedding delle frasi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "antique-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"word2index.pkl\", 'rb') as output:\n",
    "  w2i = pickle.load(output)\n",
    "with open(path+\"embedding_matrix.pkl\", 'rb') as output:\n",
    "  embedding_matrix = pickle.load(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "noted-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_trainset = np.zeros(shape=(len(sentences), max, 300))\n",
    "for i, sentence in enumerate(sentences):\n",
    "  for j, word in enumerate(sentence):\n",
    "    try:\n",
    "      embedded_trainset[i, j, :] = embedding_matrix[w2i[word]]\n",
    "    except KeyError:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "invisible-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_devset = np.zeros(shape=(len(sentences_dev), max, 300))\n",
    "for i, sentence in enumerate(sentences_dev):\n",
    "  for j, word in enumerate(sentence):\n",
    "    try:\n",
    "      embedded_devset[i, j, :] = embedding_matrix[w2i[word]]\n",
    "    except KeyError:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "technological-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_testset = np.zeros(shape=(len(sentences_test), max, 300))\n",
    "for i, sentence in enumerate(sentences_test):\n",
    "  for j, word in enumerate(sentence):\n",
    "    try:\n",
    "      embedded_testset[i, j, :] = embedding_matrix[w2i[word]]\n",
    "    except KeyError:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "everyday-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_sin = np.zeros(shape=(len(sentences_sin), max, 300))\n",
    "for i, sentence in enumerate(sentences_sin):\n",
    "  for j, word in enumerate(sentence):\n",
    "    try:\n",
    "      embedded_sin[i, j, :] = embedding_matrix[w2i[word]]\n",
    "    except KeyError:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "advance-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_emb = np.zeros(shape=(len(sentences_emb), max, 300))\n",
    "for i, sentence in enumerate(sentences_emb):\n",
    "  for j, word in enumerate(sentence):\n",
    "    try:\n",
    "      embedded_emb[i, j, :] = embedding_matrix[w2i[word]]\n",
    "    except KeyError:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-neutral",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "exempt-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = optuna.load_study(study_name=\"ATE\", storage=\"sqlite:///\"+path+\"optuna_ATE_studio_0.db\").best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "crude-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Input(shape=(max, 300)))\n",
    "model.add(keras.layers.Bidirectional(layer=keras.layers.LSTM(units=best_params[\"units\"],\n",
    "                                                             recurrent_dropout=best_params[\"dropout\"],\n",
    "                                                             activation='tanh')))\n",
    "model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=keras.optimizers.Adam(0.001),\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "material-slave",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "56/56 [==============================] - 7s 133ms/step - loss: 0.3269 - accuracy: 0.8553 - val_loss: 0.5416 - val_accuracy: 0.7706\n",
      "Epoch 2/100\n",
      "56/56 [==============================] - 7s 132ms/step - loss: 0.3035 - accuracy: 0.8779 - val_loss: 0.4626 - val_accuracy: 0.7798\n",
      "Epoch 3/100\n",
      "56/56 [==============================] - 7s 132ms/step - loss: 0.2938 - accuracy: 0.8772 - val_loss: 0.4773 - val_accuracy: 0.8073\n",
      "Epoch 4/100\n",
      "56/56 [==============================] - 7s 133ms/step - loss: 0.2869 - accuracy: 0.8818 - val_loss: 0.5636 - val_accuracy: 0.7798\n",
      "Epoch 5/100\n",
      "56/56 [==============================] - 7s 132ms/step - loss: 0.2647 - accuracy: 0.8939 - val_loss: 0.6161 - val_accuracy: 0.7523\n",
      "Epoch 6/100\n",
      "56/56 [==============================] - 7s 134ms/step - loss: 0.2429 - accuracy: 0.9024 - val_loss: 0.5216 - val_accuracy: 0.7523\n",
      "Epoch 7/100\n",
      "56/56 [==============================] - 7s 132ms/step - loss: 0.2383 - accuracy: 0.9024 - val_loss: 0.6237 - val_accuracy: 0.7706\n",
      "Epoch 8/100\n",
      "56/56 [==============================] - 7s 131ms/step - loss: 0.2048 - accuracy: 0.9214 - val_loss: 0.5415 - val_accuracy: 0.7798\n",
      "Epoch 9/100\n",
      "56/56 [==============================] - 7s 131ms/step - loss: 0.1998 - accuracy: 0.9244 - val_loss: 0.6281 - val_accuracy: 0.7615\n",
      "Epoch 10/100\n",
      "56/56 [==============================] - 7s 132ms/step - loss: 0.1886 - accuracy: 0.9276 - val_loss: 0.6868 - val_accuracy: 0.7615\n",
      "Epoch 11/100\n",
      "56/56 [==============================] - 7s 133ms/step - loss: 0.1710 - accuracy: 0.9361 - val_loss: 0.8159 - val_accuracy: 0.7523\n",
      "Epoch 12/100\n",
      "56/56 [==============================] - 7s 132ms/step - loss: 0.1813 - accuracy: 0.9250 - val_loss: 0.5200 - val_accuracy: 0.7890\n",
      "Epoch 13/100\n",
      "56/56 [==============================] - 7s 132ms/step - loss: 0.1706 - accuracy: 0.9329 - val_loss: 0.7101 - val_accuracy: 0.7798\n"
     ]
    }
   ],
   "source": [
    "result = model.fit(embedded_trainset,\n",
    "                   one_hot_train,\n",
    "                   validation_data=(embedded_devset, one_hot_dev),\n",
    "                   epochs=100,\n",
    "                   batch_size=best_params[\"batch_size\"],\n",
    "                   callbacks=[keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                                            patience=10,\n",
    "                                                            restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "boring-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('ATE_w_studio0_005.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-baghdad",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "available-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-celtic",
   "metadata": {},
   "source": [
    "## DATASET ORIGINARIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dangerous-firmware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 2s 27ms/step - loss: 0.2595 - accuracy: 0.8939\n",
      "DATASET ORIGINARIO[0.25952351093292236, 0.8939096331596375]\n"
     ]
    }
   ],
   "source": [
    "result_base = model.evaluate(embedded_trainset,one_hot_train,batch_size=best_params['batch_size'])\n",
    "print(f'DATASET ORIGINARIO{result_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "clear-noise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\tP\tN\n",
      "\n",
      "P\t640\t264\n",
      "\n",
      "N\t60\t2090\n",
      "\n",
      "\n",
      "FSCORE:\t0.9280639431616341\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(embedded_trainset, batch_size=best_params['batch_size'])\n",
    "cm = confusion_matrix(one_hot_train.argmax(axis=1), pred.argmax(axis=1))\n",
    "fscore = f1_score(one_hot_train.argmax(axis=1), pred.argmax(axis=1))\n",
    "\n",
    "print(f\"\"\"Confusion Matrix:\n",
    "\\tP\\tN\\n\n",
    "P\\t{cm[0][0]}\\t{cm[0][1]}\\n\n",
    "N\\t{cm[1][0]}\\t{cm[1][1]}\"\"\")\n",
    "\n",
    "print(f'\\n\\nFSCORE:\\t{fscore}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-croatia",
   "metadata": {},
   "source": [
    "## TESTSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "recovered-edgar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 1s 28ms/step - loss: 0.6304 - accuracy: 0.7442\n",
      "DATASET TEST[0.6304319500923157, 0.7441666722297668]\n"
     ]
    }
   ],
   "source": [
    "result_base=model.evaluate(embedded_testset,one_hot_test,batch_size=best_params['batch_size'],)\n",
    "print(f'DATASET TEST{result_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "turkish-update",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\tP\tN\n",
      "\n",
      "P\t142\t201\n",
      "\n",
      "N\t106\t751\n",
      "\n",
      "\n",
      "FSCORE:\t0.8302929795467109\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(embedded_testset, batch_size=best_params['batch_size'])\n",
    "cm = confusion_matrix(one_hot_test.argmax(axis=1), pred.argmax(axis=1))\n",
    "fscore = f1_score(one_hot_test.argmax(axis=1), pred.argmax(axis=1))\n",
    "\n",
    "print(f\"\"\"Confusion Matrix:\n",
    "\\tP\\tN\\n\n",
    "P\\t{cm[0][0]}\\t{cm[0][1]}\\n\n",
    "N\\t{cm[1][0]}\\t{cm[1][1]}\"\"\")\n",
    "\n",
    "print(f'\\n\\nFSCORE:\\t{fscore}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-template",
   "metadata": {},
   "source": [
    "## DATASET SINONIMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "established-ideal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 1s 27ms/step - loss: 0.6409 - accuracy: 0.7400\n",
      "DATASET SINONIMI[0.6409004330635071, 0.7400000095367432]\n"
     ]
    }
   ],
   "source": [
    "result_base=model.evaluate(embedded_sin,one_hot_sin,batch_size=best_params['batch_size'])\n",
    "print(f'DATASET SINONIMI{result_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "female-lounge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\tP\tN\n",
      "\n",
      "P\t139\t204\n",
      "\n",
      "N\t108\t749\n",
      "\n",
      "\n",
      "FSCORE:\t0.8276243093922652\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(embedded_sin, batch_size=best_params['batch_size'])\n",
    "cm = confusion_matrix(one_hot_sin.argmax(axis=1), pred.argmax(axis=1))\n",
    "fscore = f1_score(one_hot_sin.argmax(axis=1), pred.argmax(axis=1))\n",
    "\n",
    "print(f\"\"\"Confusion Matrix:\n",
    "\\tP\\tN\\n\n",
    "P\\t{cm[0][0]}\\t{cm[0][1]}\\n\n",
    "N\\t{cm[1][0]}\\t{cm[1][1]}\"\"\")\n",
    "\n",
    "print(f'\\n\\nFSCORE:\\t{fscore}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-spider",
   "metadata": {},
   "source": [
    "## DATASET EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dynamic-valve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 1s 27ms/step - loss: 0.6281 - accuracy: 0.7442\n",
      "DATASET EMBEDDING[0.6281087398529053, 0.7441666722297668]\n"
     ]
    }
   ],
   "source": [
    "result_base=model.evaluate(embedded_emb,one_hot_emb,batch_size=best_params['batch_size'])\n",
    "print(f'DATASET EMBEDDING{result_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "toxic-husband",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\tP\tN\n",
      "\n",
      "P\t141\t202\n",
      "\n",
      "N\t105\t752\n",
      "\n",
      "\n",
      "FSCORE:\t0.8304803975704032\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(embedded_emb, batch_size=best_params['batch_size'])\n",
    "cm = confusion_matrix(one_hot_emb.argmax(axis=1), pred.argmax(axis=1))\n",
    "fscore = f1_score(one_hot_emb.argmax(axis=1), pred.argmax(axis=1))\n",
    "\n",
    "print(f\"\"\"Confusion Matrix:\n",
    "\\tP\\tN\\n\n",
    "P\\t{cm[0][0]}\\t{cm[0][1]}\\n\n",
    "N\\t{cm[1][0]}\\t{cm[1][1]}\"\"\")\n",
    "\n",
    "print(f'\\n\\nFSCORE:\\t{fscore}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
