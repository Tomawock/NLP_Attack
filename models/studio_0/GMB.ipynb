{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GMB.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tomawock/NLP_Attack/blob/main/models/studio_0/GMB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POK6j5oiTzzi"
      },
      "source": [
        "###Studio 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4D6g8HCVCnq"
      },
      "source": [
        "!pip install h5py\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLcY-sL2EW4T"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EsqztNaYcQE"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from sklearn import preprocessing\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCjC_sLPVsC7"
      },
      "source": [
        "data_original = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_dataset.tsv\"\n",
        "max_len = 384"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rsWWOi_YPtO"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False, reduction=tf.keras.losses.Reduction.SUM\n",
        ")\n",
        "\n",
        "# Ignoring loss that is calculated due to padded targets\n",
        "def masked_ce_loss(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 17))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "def create_model(num_tags):\n",
        "    # BERT Base model\n",
        "    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # NER Model\n",
        "\n",
        "    # Instantiate Keras tensors\n",
        "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    # TFBertmodel\n",
        "    embedding = encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
        "    # Add drop out layer\n",
        "    embedding = layers.Dropout(0.5)(embedding)\n",
        "    # Add softmax layer for classifying\n",
        "    tag_logits = layers.Dense(num_tags+1, activation='softmax')(embedding)\n",
        "    \n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask],\n",
        "        outputs=[tag_logits],\n",
        "    )\n",
        "    optimizer = keras.optimizers.Adam(lr=3e-5)\n",
        "    model.compile(optimizer=optimizer, loss=masked_ce_loss, metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl2kaHNtcO-F"
      },
      "source": [
        "#Reference - https://keras.io/examples/nlp/text_extraction_with_bert/\n",
        "# Save the slow pretrained tokenizer\n",
        "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "save_path = \"bert_base_uncased/\"\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "slow_tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Load the fast tokenizer from saved file\n",
        "tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBO2axNIFmXH"
      },
      "source": [
        "MUST USE GPU or model dont load "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQxXGab4Vg5U"
      },
      "source": [
        "parz = pd.read_csv(data_original,sep='\\t', encoding=\"latin-1\", index_col=[0])\n",
        "\n",
        "num_tags = parz[\"Tag\"].nunique()\n",
        "\n",
        "new_model = create_model(num_tags)\n",
        "    \n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mssGj3zjI2hh"
      },
      "source": [
        "###Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKYEp_xsaQJ4"
      },
      "source": [
        "# Get the sentences\n",
        "def process_csv(data_path):\n",
        "    df = pd.read_csv(data_path, encoding=\"latin-1\", na_filter = False)\n",
        "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
        "    enc_tag = preprocessing.LabelEncoder()\n",
        "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
        "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "    return sentences, tag, enc_tag\n",
        "\n",
        "# Get the sentences\n",
        "def process_tsv(data_path):\n",
        "    df =  pd.read_csv(data_path,sep='\\t', encoding=\"latin-1\", index_col=[0])\n",
        "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
        "    enc_tag = preprocessing.LabelEncoder()\n",
        "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
        "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "    return sentences, tag, enc_tag\n",
        "\n",
        "# Prepare the inputs for feeding into model\n",
        "def create_inputs_targets(data):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"tags\": []\n",
        "    }\n",
        "    _,ext=os.path.splitext(data)\n",
        "    if (ext== '.tsv'):\n",
        "      sentences, tags, tag_encoder = process_tsv(data)\n",
        "    else:\n",
        "      sentences, tags, tag_encoder = process_csv(data)\n",
        "    \n",
        "    for sentence, tag in zip(sentences, tags):\n",
        "        \n",
        "        input_ids = []\n",
        "        target_tags = []\n",
        "        for idx, word in enumerate(sentence):\n",
        "            ids = tokenizer.encode(word, add_special_tokens=False)\n",
        "            input_ids.extend(ids.ids)\n",
        "            num_tokens = len(ids)\n",
        "            target_tags.extend([tag[idx]] * num_tokens)\n",
        "                    \n",
        "        \n",
        "        # Pad truncate\n",
        "        input_ids = input_ids[:max_len - 2]\n",
        "        target_tags = target_tags[:max_len - 2]\n",
        "\n",
        "        # Add [CLS] and [SEP]\n",
        "        input_ids = [101] + input_ids + [102]\n",
        "        target_tags = [16] + target_tags + [16]\n",
        "        # token_type_ids does not matter as the task has only one sentence\n",
        "        token_type_ids = [0] * len(input_ids)\n",
        "        # Adding attention mask for non-padded input\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        \n",
        "        # Making all the input enbedding same lenting by padding\n",
        "        padding_len = max_len - len(input_ids)\n",
        "        input_ids = input_ids + ([0] * padding_len)\n",
        "        attention_mask = attention_mask + ([0] * padding_len)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "        target_tags = target_tags + ([17] * padding_len)\n",
        "        \n",
        "        dataset_dict[\"input_ids\"].append(input_ids)\n",
        "        dataset_dict[\"token_type_ids\"].append(token_type_ids)\n",
        "        dataset_dict[\"attention_mask\"].append(attention_mask)\n",
        "        dataset_dict[\"tags\"].append(target_tags)\n",
        "        assert len(target_tags) == max_len, f'{len(input_ids)}, {len(target_tags)}'\n",
        "                \n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    # Creating array of input embeddings\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"token_type_ids\"],\n",
        "        dataset_dict[\"attention_mask\"],\n",
        "    ]\n",
        "    y = dataset_dict[\"tags\"]\n",
        "    return x, y, tag_encoder"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y36IkdBRPaev"
      },
      "source": [
        "data_original_train = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_original_train.csv\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGeGXmfcI1jU"
      },
      "source": [
        "x_train, y_train, tag_encoder = create_inputs_targets(data_original_train)\n",
        "\n",
        "bs = 16\n",
        "\n",
        "new_model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=1,\n",
        "    verbose=1,\n",
        "    batch_size=bs,\n",
        "    validation_split=0.1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF-oLRMgV65C"
      },
      "source": [
        "new_model.save_weights(\"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_w_studio_0_model.h5\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u1o8pPySLR2"
      },
      "source": [
        "###Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-J7Xft_VWVT"
      },
      "source": [
        "new_model.load_weights(\"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_w_studio_0_model.h5\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkQHJBaBdfFU"
      },
      "source": [
        "datapath_original_test = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_original_test.csv\"\n",
        "datapath_sinonimi_test = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_sinonimi_test.csv\"\n",
        "datapath_embedding_test = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_embedding_test.csv\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cQdMq8Re3Qb"
      },
      "source": [
        "Original\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFc4tfOHe5RE",
        "outputId": "0c4b77d5-429d-4b3e-f485-eb5928db634e"
      },
      "source": [
        "#crea i dati per essere elborati dal modello\n",
        "x_test, y_test, tag_encoder = create_inputs_targets(datapath_original_test)\n",
        "pred_test = new_model.evaluate(x_test,y_test,batch_size=16)\n",
        "#definisce il dizioanrio dell'associazione classi valore numerico"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 25s 437ms/step - loss: 222.4186 - accuracy: 0.9399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-DCDEnySdno"
      },
      "source": [
        "Sinonimi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVfSNN8vV6-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e400bf07-9f84-4f4a-97c5-0861403dbb95"
      },
      "source": [
        "#crea i dati per essere elborati dal modello\n",
        "x_test, y_test, tag_encoder = create_inputs_targets(datapath_sinonimi_test)\n",
        "pred_test = new_model.evaluate(x_test,y_test,batch_size=16)\n",
        "#definisce il dizioanrio dell'associazione classi valore numerico"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 25s 439ms/step - loss: 297.2466 - accuracy: 0.9272\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4HTD1kTSf89"
      },
      "source": [
        "Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT_yp3H2QTpf",
        "outputId": "e7defb83-69f1-4c83-d372-c52b88149cdd"
      },
      "source": [
        "#crea i dati per essere elborati dal modello\n",
        "x_test, y_test, tag_encoder = create_inputs_targets(datapath_embedding_test)\n",
        "pred_test = new_model.evaluate(x_test,y_test,batch_size=16)\n",
        "#definisce il dizioanrio dell'associazione classi valore numerico"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 25s 433ms/step - loss: 270.7449 - accuracy: 0.9312\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}