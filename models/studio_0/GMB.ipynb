{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GMB.ipynb",
      "provenance": [],
      "mount_file_id": "1LeMqcZUK8Gz9oG6K9UUwqKRyqhUv0alX",
      "authorship_tag": "ABX9TyMbL70xNEydnlunJBJdKRfv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tomawock/NLP_Attack/blob/main/models/studio_0/GMB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POK6j5oiTzzi"
      },
      "source": [
        "Parte nuova testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4D6g8HCVCnq"
      },
      "source": [
        "!pip install h5py\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EsqztNaYcQE"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from sklearn import preprocessing\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "import numpy as np"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCjC_sLPVsC7"
      },
      "source": [
        "data_original = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_dataset.tsv\"\n",
        "data_sinonimi = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_sinonimi.csv\"\n",
        "data_embedding = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_embedding.csv\"\n",
        "max_len = 384"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rsWWOi_YPtO"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False, reduction=tf.keras.losses.Reduction.SUM\n",
        ")\n",
        "\n",
        "# Ignoring loss that is calculated due to padded targets\n",
        "def masked_ce_loss(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 17))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "def create_model(num_tags):\n",
        "    # BERT Base model\n",
        "    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # NER Model\n",
        "\n",
        "    # Instantiate Keras tensors\n",
        "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    # TFBertmodel\n",
        "    embedding = encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
        "    # Add drop out layer\n",
        "    embedding = layers.Dropout(0.5)(embedding)\n",
        "    # Add softmax layer for classifying\n",
        "    tag_logits = layers.Dense(num_tags+1, activation='softmax')(embedding)\n",
        "    \n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask],\n",
        "        outputs=[tag_logits],\n",
        "    )\n",
        "    optimizer = keras.optimizers.Adam(lr=3e-5)\n",
        "    model.compile(optimizer=optimizer, loss=masked_ce_loss, metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQxXGab4Vg5U"
      },
      "source": [
        "parz = pd.read_csv(data_original,sep='\\t', encoding=\"latin-1\", index_col=[0])\n",
        "\n",
        "num_tags = parz[\"Tag\"].nunique()\n",
        "\n",
        "use_tpu = None\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    use_tpu = True\n",
        "except:\n",
        "    use_tpu = False\n",
        "\n",
        "if use_tpu:\n",
        "    # Create distribution strategy\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "    # Create model\n",
        "    with strategy.scope():\n",
        "        new_model = create_model(num_tags)\n",
        "else:\n",
        "    new_model = create_model(num_tags)\n",
        "    \n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-J7Xft_VWVT"
      },
      "source": [
        "new_model.load_weights(\"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_w_model.h5\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UgD9wFWQMV"
      },
      "source": [
        "sinonimi=pd.read_csv(data_sinonimi, encoding=\"latin-1\", index_col=[0])\n",
        "embedding=pd.read_csv(data_embedding, encoding=\"latin-1\", na_filter = False)\n",
        "original=pd.read_csv(data_original,sep='\\t', encoding=\"latin-1\")"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgFiQjD-Z0mO"
      },
      "source": [
        "sinonimi.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blvQK5xEmwAK"
      },
      "source": [
        "original.loc[original[\"Word\"] == \"NASA\", ['Sentence #', 'Word']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlkkvxq2nnlo"
      },
      "source": [
        "original.loc[original[\"Sentence #\"] == 2555.0, ['Sentence #', 'Word']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeHZuKYciJNA"
      },
      "source": [
        "[i for i, parola in enumerate(embedding.Word) if not isinstance(parola, str)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBziWU1Vjzuv"
      },
      "source": [
        "embedding.iloc[[56409]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZr4vfNXlJ_t"
      },
      "source": [
        "embedding[\"Sentence #\"] == 5553.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8tk-C4ykbye"
      },
      "source": [
        "embedding.loc[embedding[\"Sentence #\"] == 5553.0, ['Word']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKYEp_xsaQJ4"
      },
      "source": [
        "# Get the sentences\n",
        "def process_csv(data_path):\n",
        "    df = pd.read_csv(data_path, encoding=\"latin-1\", na_filter = False)\n",
        "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
        "    enc_tag = preprocessing.LabelEncoder()\n",
        "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
        "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "    return sentences, tag, enc_tag\n",
        "\n",
        "# Prepare the inputs for feeding into model\n",
        "def create_inputs_targets(data):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"tags\": []\n",
        "    }\n",
        "    sentences, tags, tag_encoder = process_csv(data)\n",
        "    print(sentences[0])\n",
        "    \n",
        "    for sentence, tag in zip(sentences, tags):\n",
        "        \n",
        "        input_ids = []\n",
        "        target_tags = []\n",
        "        for idx, word in enumerate(sentence):\n",
        "            ids = tokenizer.encode(word, add_special_tokens=False)\n",
        "            input_ids.extend(ids.ids)\n",
        "            num_tokens = len(ids)\n",
        "            target_tags.extend([tag[idx]] * num_tokens)\n",
        "                    \n",
        "        \n",
        "        # Pad truncate\n",
        "        input_ids = input_ids[:max_len - 2]\n",
        "        target_tags = target_tags[:max_len - 2]\n",
        "\n",
        "        # Add [CLS] and [SEP]\n",
        "        input_ids = [101] + input_ids + [102]\n",
        "        target_tags = [16] + target_tags + [16]\n",
        "        # token_type_ids does not matter as the task has only one sentence\n",
        "        token_type_ids = [0] * len(input_ids)\n",
        "        # Adding attention mask for non-padded input\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        \n",
        "        # Making all the input enbedding same lenting by padding\n",
        "        padding_len = max_len - len(input_ids)\n",
        "        input_ids = input_ids + ([0] * padding_len)\n",
        "        attention_mask = attention_mask + ([0] * padding_len)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "        target_tags = target_tags + ([17] * padding_len)\n",
        "        \n",
        "        dataset_dict[\"input_ids\"].append(input_ids)\n",
        "        dataset_dict[\"token_type_ids\"].append(token_type_ids)\n",
        "        dataset_dict[\"attention_mask\"].append(attention_mask)\n",
        "        dataset_dict[\"tags\"].append(target_tags)\n",
        "        assert len(target_tags) == max_len, f'{len(input_ids)}, {len(target_tags)}'\n",
        "                \n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    # Creating array of input embeddings\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"token_type_ids\"],\n",
        "        dataset_dict[\"attention_mask\"],\n",
        "    ]\n",
        "    y = dataset_dict[\"tags\"]\n",
        "    return x, y, tag_encoder"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVfSNN8vV6-5"
      },
      "source": [
        "#crea i dati per essere elborati dal modello\n",
        "x_test, y_test, n_tokens = create_inputs_targets(data_sinonimi)\n",
        "pred_test = new_model.evaluate(x_test,y_test)\n",
        "#definisce il dizioanrio dell'associazione classi valore numerico"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ3ZF8GCbQ-x"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tpp2u2-bNtH"
      },
      "source": [
        "le_dict = dict(zip(tag_encoder.transform(tag_encoder.classes_), tag_encoder.classes_))\n",
        "result =[]\n",
        "for i in range(len(test_inputs)):\n",
        "  pred_tags = np.argmax(pred_test,2)[i][:n_tokens[i]]#trova il vettore delle classsi associato ad ongi stringa in ingresso\n",
        "  pred_tags = [le_dict.get(_, '[pad]') for _ in pred_tags]\n",
        "  result.append(pred_tags)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
