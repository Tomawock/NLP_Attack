{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nearby-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "healthy-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fourth-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "greater-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_org = pd.read_json(path+'ATE_ABSITA_training_set/ate_absita_training.ndjson', lines=True)\n",
    "train_sin = pd.read_csv(path+'ATE_ABSITA_training_set/sinonimi.csv')\n",
    "train_emb = pd.read_csv(path+'ATE_ABSITA_training_set/embedding.csv')\n",
    "\n",
    "dev = pd.concat([pd.read_json(path+'ATE_ABSITA_dev_set/ate_absita_dev.ndjson', lines=True),\n",
    "                 pd.read_csv(path+'ATE_ABSITA_dev_set/sinonimi.csv'),\n",
    "                 pd.read_csv(path+'ATE_ABSITA_dev_set/embedding.csv')],\n",
    "                ignore_index=True)\n",
    "\n",
    "test_org = pd.read_json(path+'ATE_ABSITA_test_set/ate_absita_gold.ndjson', lines=True)\n",
    "test_sin = pd.read_csv(path+'ATE_ABSITA_test_set/sinonimi.csv')\n",
    "test_emb = pd.read_csv(path+'ATE_ABSITA_test_set/embedding.csv')\n",
    "\n",
    "\n",
    "data_sinonimi = pd.read_csv(path+\"ATE_ABSITA_test_set/sinonimi.csv\")\n",
    "data_embedding = pd.read_csv(path+\"ATE_ABSITA_test_set/embedding.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "competent-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_study_1 = pd.concat([train_org, train_sin, train_emb], ignore_index=True).sample(frac=1)\n",
    "test_study_1 = pd.concat([test_org, test_sin, test_emb], ignore_index=True).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "surprising-reservoir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contains 9162 sentences\n",
      "Contains 327 sentences\n",
      "Contains 3600 sentences\n",
      "Contains 1200 sentences\n",
      "Contains 1200 sentences\n"
     ]
    }
   ],
   "source": [
    "train_study_1.drop(columns=['id_sentence','polarities','aspects_position','aspects'], inplace=True)\n",
    "dev.drop(columns=['id_sentence','polarities','aspects_position','aspects'], inplace=True)\n",
    "test_org.drop(columns=['id_sentence','polarities','aspects_position','aspects'], inplace=True)\n",
    "test_study_1.drop(columns=['id_sentence','polarities','aspects_position','aspects'], inplace=True)\n",
    "data_sinonimi.drop(columns=['polarities','aspects_position','aspects'], inplace=True)\n",
    "data_embedding.drop(columns=['polarities','aspects_position','aspects'], inplace=True)\n",
    "\n",
    "print(f'Contains {len(train_study_1)} sentences')\n",
    "print(f'Contains {len(dev)} sentences')\n",
    "print(f'Contains {len(test_study_1)} sentences')\n",
    "print(f'Contains {len(data_sinonimi)} sentences')\n",
    "print(f'Contains {len(data_embedding)} sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "according-revelation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN::\n",
      "pos    6450\n",
      "neg    2712\n",
      "Name: review_type, dtype: int64\n",
      "DEV::\n",
      "pos    258\n",
      "neg     69\n",
      "Name: review_type, dtype: int64\n",
      "TEST::\n",
      "pos    2571\n",
      "neg    1029\n",
      "Name: review_type, dtype: int64\n",
      "SINONIMI::\n",
      "pos    857\n",
      "neg    343\n",
      "Name: review_type, dtype: int64\n",
      "EMBEDDING::\n",
      "pos    857\n",
      "neg    343\n",
      "Name: review_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_study_1[\"review_type\"] = train_study_1[\"score\"].apply(lambda x: \"neg\" if x < 5 else \"pos\")\n",
    "dev[\"review_type\"] = dev[\"score\"].apply(lambda x: \"neg\" if x < 5 else \"pos\")\n",
    "test_org[\"review_type\"] = test_org[\"score\"].apply(lambda x: \"neg\" if x < 5 else \"pos\")\n",
    "test_study_1[\"review_type\"] = test_study_1[\"score\"].apply(lambda x: \"neg\" if x < 5 else \"pos\")\n",
    "data_sinonimi[\"review_type\"] = data_sinonimi[\"score\"].apply(lambda x: \"neg\" if x < 5 else \"pos\")\n",
    "data_embedding[\"review_type\"] = data_embedding[\"score\"].apply(lambda x: \"neg\" if x < 5 else \"pos\")\n",
    "\n",
    "print(f'TRAIN::\\n{train_study_1.review_type.value_counts()}')\n",
    "print(f'DEV::\\n{dev.review_type.value_counts()}')\n",
    "print(f'TEST::\\n{test_study_1.review_type.value_counts()}')\n",
    "print(f'SINONIMI::\\n{data_sinonimi.review_type.value_counts()}')\n",
    "print(f'EMBEDDING::\\n{data_embedding.review_type.value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "parental-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_study_1.drop(columns=['score'], inplace=True)\n",
    "dev.drop(columns=['score'], inplace=True)\n",
    "test_org.drop(columns=['score'], inplace=True)\n",
    "test_study_1.drop(columns=['score'], inplace=True)\n",
    "data_sinonimi.drop(columns=['score'], inplace=True)\n",
    "data_embedding.drop(columns=['score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "precious-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_text_to_word_sequence(sentence):\n",
    "    return keras.preprocessing.text.text_to_word_sequence(sentence,\n",
    "                                                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`\\'{|}~\\t\\n',\n",
    "                                                          lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-washington",
   "metadata": {},
   "source": [
    "# OneHotEncode delle frasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "light-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_train = OneHotEncoder(sparse=False).fit_transform(\n",
    "        train_study_1.review_type.to_numpy().reshape(-1, 1))\n",
    "\n",
    "sentences = [my_text_to_word_sequence(sentence) for sentence in train_study_1['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fewer-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_dev = OneHotEncoder(sparse=False).fit_transform(\n",
    "        dev.review_type.to_numpy().reshape(-1, 1))\n",
    "\n",
    "sentences_dev = [my_text_to_word_sequence(sentence) for sentence in dev['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "differential-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_test_org = OneHotEncoder(sparse=False).fit_transform(\n",
    "        test_org.review_type.to_numpy().reshape(-1, 1))\n",
    "\n",
    "sentences_test_org = [my_text_to_word_sequence(sentence) for sentence in test_org['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pretty-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_test = OneHotEncoder(sparse=False).fit_transform(\n",
    "        test_study_1.review_type.to_numpy().reshape(-1, 1))\n",
    "\n",
    "sentences_test = [my_text_to_word_sequence(sentence) for sentence in test_study_1['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "activated-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_sin = OneHotEncoder(sparse=False).fit_transform(\n",
    "        data_sinonimi.review_type.to_numpy().reshape(-1, 1))\n",
    "\n",
    "sentences_sin = [my_text_to_word_sequence(sentence) for sentence in data_sinonimi['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "radical-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_emb = OneHotEncoder(sparse=False).fit_transform(\n",
    "        data_embedding.review_type.to_numpy().reshape(-1, 1))\n",
    "\n",
    "sentences_emb = [my_text_to_word_sequence(sentence) for sentence in data_embedding['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "illegal-phone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il massimo è 86\n"
     ]
    }
   ],
   "source": [
    "max_index, max = (-1, -1)\n",
    "for i, sentence in enumerate(sentences):\n",
    "  max_index, max = (i, len(sentence)) if len(sentence) > max else (max_index, max)\n",
    "for i, sentence in enumerate(sentences_dev):\n",
    "  max_index, max = (i, len(sentence)) if len(sentence) > max else (max_index, max)\n",
    "for i, sentence in enumerate(sentences_test):\n",
    "  max_index, max = (i, len(sentence)) if len(sentence) > max else (max_index, max)\n",
    "for i, sentence in enumerate(sentences_sin):\n",
    "  max_index, max = (i, len(sentence)) if len(sentence) > max else (max_index, max)\n",
    "for i, sentence in enumerate(sentences_emb):\n",
    "  max_index, max = (i, len(sentence)) if len(sentence) > max else (max_index, max)\n",
    "\n",
    "\n",
    "print(f'Il massimo è {max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-convertible",
   "metadata": {},
   "source": [
    "# Embedding delle frasi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "antique-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"word2index.pkl\", 'rb') as output:\n",
    "  w2i = pickle.load(output)\n",
    "with open(path+\"embedding_matrix.pkl\", 'rb') as output:\n",
    "  embedding_matrix = pickle.load(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "noted-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_trainset = np.zeros(shape=(len(sentences), max, 300))\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for j, word in enumerate(sentence):\n",
    "        try:\n",
    "            embedded_trainset[i, j, :] = embedding_matrix[w2i[word]]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "invisible-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_devset = np.zeros(shape=(len(sentences_dev), max, 300))\n",
    "for i, sentence in enumerate(sentences_dev):\n",
    "    for j, word in enumerate(sentence):\n",
    "        try:\n",
    "            embedded_devset[i, j, :] = embedding_matrix[w2i[word]]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "completed-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_testset_org = np.zeros(shape=(len(sentences_test_org), max, 300))\n",
    "for i, sentence in enumerate(sentences_test_org):\n",
    "    for j, word in enumerate(sentence):\n",
    "        try:\n",
    "            embedded_testset_org[i, j, :] = embedding_matrix[w2i[word]]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "technological-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_testset = np.zeros(shape=(len(sentences_test), max, 300))\n",
    "for i, sentence in enumerate(sentences_test):\n",
    "    for j, word in enumerate(sentence):\n",
    "        try:\n",
    "            embedded_testset[i, j, :] = embedding_matrix[w2i[word]]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "everyday-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_sin = np.zeros(shape=(len(sentences_sin), max, 300))\n",
    "for i, sentence in enumerate(sentences_sin):\n",
    "    for j, word in enumerate(sentence):\n",
    "        try:\n",
    "            embedded_sin[i, j, :] = embedding_matrix[w2i[word]]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "advance-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_emb = np.zeros(shape=(len(sentences_emb), max, 300))\n",
    "for i, sentence in enumerate(sentences_emb):\n",
    "    for j, word in enumerate(sentence):\n",
    "        try:\n",
    "            embedded_emb[i, j, :] = embedding_matrix[w2i[word]]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-neutral",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "exempt-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = optuna.load_study(study_name=\"ATE\", storage=\"sqlite:///\"+path+\"optuna_ATE_studio_0.db\").best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "crude-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Input(shape=(max, 300)))\n",
    "model.add(keras.layers.Bidirectional(layer=keras.layers.LSTM(units=best_params[\"units\"],\n",
    "                                                             recurrent_dropout=best_params[\"dropout\"],\n",
    "                                                             activation='tanh')))\n",
    "model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=keras.optimizers.Adam(0.001),\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "material-slave",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "167/167 [==============================] - 26s 134ms/step - loss: 0.5997 - accuracy: 0.6936 - val_loss: 0.5138 - val_accuracy: 0.7615\n",
      "Epoch 2/100\n",
      "167/167 [==============================] - 22s 133ms/step - loss: 0.5121 - accuracy: 0.7405 - val_loss: 0.4255 - val_accuracy: 0.8318\n",
      "Epoch 3/100\n",
      "167/167 [==============================] - 22s 132ms/step - loss: 0.4628 - accuracy: 0.7846 - val_loss: 0.4405 - val_accuracy: 0.7920\n",
      "Epoch 4/100\n",
      "167/167 [==============================] - 22s 132ms/step - loss: 0.4299 - accuracy: 0.8045 - val_loss: 0.3958 - val_accuracy: 0.8440\n",
      "Epoch 5/100\n",
      "167/167 [==============================] - 22s 133ms/step - loss: 0.3768 - accuracy: 0.8357 - val_loss: 0.4338 - val_accuracy: 0.7951\n",
      "Epoch 6/100\n",
      "167/167 [==============================] - 22s 132ms/step - loss: 0.3334 - accuracy: 0.8579 - val_loss: 0.5017 - val_accuracy: 0.8043\n",
      "Epoch 7/100\n",
      "167/167 [==============================] - 22s 133ms/step - loss: 0.2865 - accuracy: 0.8818 - val_loss: 0.6166 - val_accuracy: 0.7676\n",
      "Epoch 8/100\n",
      "167/167 [==============================] - 22s 133ms/step - loss: 0.2343 - accuracy: 0.9041 - val_loss: 0.6086 - val_accuracy: 0.7584\n",
      "Epoch 9/100\n",
      "167/167 [==============================] - 22s 133ms/step - loss: 0.2008 - accuracy: 0.9190 - val_loss: 0.6687 - val_accuracy: 0.7706\n",
      "Epoch 10/100\n",
      "167/167 [==============================] - 22s 133ms/step - loss: 0.1708 - accuracy: 0.9329 - val_loss: 0.7324 - val_accuracy: 0.7401\n",
      "Epoch 11/100\n",
      "167/167 [==============================] - 22s 134ms/step - loss: 0.1415 - accuracy: 0.9467 - val_loss: 0.6790 - val_accuracy: 0.7737\n",
      "Epoch 12/100\n",
      "167/167 [==============================] - 22s 133ms/step - loss: 0.1123 - accuracy: 0.9572 - val_loss: 0.6049 - val_accuracy: 0.7829\n",
      "Epoch 13/100\n",
      "167/167 [==============================] - 22s 132ms/step - loss: 0.1038 - accuracy: 0.9611 - val_loss: 0.7305 - val_accuracy: 0.7859\n",
      "Epoch 14/100\n",
      "167/167 [==============================] - 22s 132ms/step - loss: 0.1089 - accuracy: 0.9602 - val_loss: 0.9904 - val_accuracy: 0.7370\n"
     ]
    }
   ],
   "source": [
    "result = model.fit(embedded_trainset,\n",
    "                   one_hot_train,\n",
    "                   validation_data=(embedded_devset, one_hot_dev),\n",
    "                   epochs=100,\n",
    "                   batch_size=best_params[\"batch_size\"],\n",
    "                   callbacks=[keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                                            patience=10,\n",
    "                                                            restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "boring-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('ATE_w_studio3_005.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-baghdad",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "available-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-celtic",
   "metadata": {},
   "source": [
    "## DATASET ORIGINARIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dangerous-firmware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 5s 27ms/step - loss: 0.3719 - accuracy: 0.8351\n",
      "DATASET ORIGINARIO[0.371893048286438, 0.8350796699523926]\n"
     ]
    }
   ],
   "source": [
    "result_base = model.evaluate(embedded_trainset,one_hot_train,batch_size=best_params['batch_size'])\n",
    "print(f'DATASET ORIGINARIO{result_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "clear-noise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\tP\tN\n",
      "\n",
      "P\t1464\t1248\n",
      "\n",
      "N\t263\t6187\n",
      "\n",
      "\n",
      "FSCORE:\t0.8911775297083183\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(embedded_trainset, batch_size=best_params['batch_size'])\n",
    "cm = confusion_matrix(one_hot_train.argmax(axis=1), pred.argmax(axis=1))\n",
    "fscore = f1_score(one_hot_train.argmax(axis=1), pred.argmax(axis=1))\n",
    "\n",
    "print(f\"\"\"Confusion Matrix:\n",
    "\\tP\\tN\\n\n",
    "P\\t{cm[0][0]}\\t{cm[0][1]}\\n\n",
    "N\\t{cm[1][0]}\\t{cm[1][1]}\"\"\")\n",
    "\n",
    "print(f'\\n\\nFSCORE:\\t{fscore}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-croatia",
   "metadata": {},
   "source": [
    "## TESTSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "recovered-edgar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 2s 27ms/step - loss: 0.5562 - accuracy: 0.7697\n",
      "DATASET TEST[0.5561699867248535, 0.7697222232818604]\n"
     ]
    }
   ],
   "source": [
    "result_base=model.evaluate(embedded_testset,one_hot_test,batch_size=best_params['batch_size'])\n",
    "print(f'DATASET TEST{result_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "turkish-update",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\tP\tN\n",
      "\n",
      "P\t420\t609\n",
      "\n",
      "N\t220\t2351\n",
      "\n",
      "\n",
      "FSCORE:\t0.8501175194359067\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(embedded_testset, batch_size=best_params['batch_size'])\n",
    "cm = confusion_matrix(one_hot_test.argmax(axis=1), pred.argmax(axis=1))\n",
    "fscore = f1_score(one_hot_test.argmax(axis=1), pred.argmax(axis=1))\n",
    "\n",
    "print(f\"\"\"Confusion Matrix:\n",
    "\\tP\\tN\\n\n",
    "P\\t{cm[0][0]}\\t{cm[0][1]}\\n\n",
    "N\\t{cm[1][0]}\\t{cm[1][1]}\"\"\")\n",
    "\n",
    "print(f'\\n\\nFSCORE:\\t{fscore}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-initial",
   "metadata": {},
   "source": [
    "## TESTSET ORIGINARIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "resistant-century",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 1s 27ms/step - loss: 0.5558 - accuracy: 0.7725\n",
      "DATASET TEST[0.5558383464813232, 0.7724999785423279]\n"
     ]
    }
   ],
   "source": [
    "result_base=model.evaluate(embedded_testset_org,one_hot_test_org,batch_size=best_params['batch_size'])\n",
    "print(f'DATASET TEST{result_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "convertible-diameter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\tP\tN\n",
      "\n",
      "P\t142\t201\n",
      "\n",
      "N\t72\t785\n",
      "\n",
      "\n",
      "FSCORE:\t0.8518719479110147\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(embedded_testset_org, batch_size=best_params['batch_size'])\n",
    "cm = confusion_matrix(one_hot_test_org.argmax(axis=1), pred.argmax(axis=1))\n",
    "fscore = f1_score(one_hot_test_org.argmax(axis=1), pred.argmax(axis=1))\n",
    "\n",
    "print(f\"\"\"Confusion Matrix:\n",
    "\\tP\\tN\\n\n",
    "P\\t{cm[0][0]}\\t{cm[0][1]}\\n\n",
    "N\\t{cm[1][0]}\\t{cm[1][1]}\"\"\")\n",
    "\n",
    "print(f'\\n\\nFSCORE:\\t{fscore}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-template",
   "metadata": {},
   "source": [
    "## DATASET SINONIMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "established-ideal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 1s 27ms/step - loss: 0.5569 - accuracy: 0.7667\n",
      "DATASET SINONIMI[0.556877613067627, 0.7666666507720947]\n"
     ]
    }
   ],
   "source": [
    "result_base=model.evaluate(embedded_sin,one_hot_sin,batch_size=best_params['batch_size'])\n",
    "print(f'DATASET SINONIMI{result_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "female-lounge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\tP\tN\n",
      "\n",
      "P\t135\t208\n",
      "\n",
      "N\t72\t785\n",
      "\n",
      "\n",
      "FSCORE:\t0.8486486486486486\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(embedded_sin, batch_size=best_params['batch_size'])\n",
    "cm = confusion_matrix(one_hot_sin.argmax(axis=1), pred.argmax(axis=1))\n",
    "fscore = f1_score(one_hot_sin.argmax(axis=1), pred.argmax(axis=1))\n",
    "\n",
    "print(f\"\"\"Confusion Matrix:\n",
    "\\tP\\tN\\n\n",
    "P\\t{cm[0][0]}\\t{cm[0][1]}\\n\n",
    "N\\t{cm[1][0]}\\t{cm[1][1]}\"\"\")\n",
    "\n",
    "print(f'\\n\\nFSCORE:\\t{fscore}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-spider",
   "metadata": {},
   "source": [
    "## DATASET EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dynamic-valve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 1s 28ms/step - loss: 0.5558 - accuracy: 0.7700\n",
      "DATASET EMBEDDING[0.5557941198348999, 0.7699999809265137]\n"
     ]
    }
   ],
   "source": [
    "result_base=model.evaluate(embedded_emb,one_hot_emb,batch_size=best_params['batch_size'])\n",
    "print(f'DATASET EMBEDDING{result_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "toxic-husband",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\tP\tN\n",
      "\n",
      "P\t143\t200\n",
      "\n",
      "N\t76\t781\n",
      "\n",
      "\n",
      "FSCORE:\t0.8498367791077257\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(embedded_emb, batch_size=best_params['batch_size'])\n",
    "cm = confusion_matrix(one_hot_emb.argmax(axis=1), pred.argmax(axis=1))\n",
    "fscore = f1_score(one_hot_emb.argmax(axis=1), pred.argmax(axis=1))\n",
    "\n",
    "print(f\"\"\"Confusion Matrix:\n",
    "\\tP\\tN\\n\n",
    "P\\t{cm[0][0]}\\t{cm[0][1]}\\n\n",
    "N\\t{cm[1][0]}\\t{cm[1][1]}\"\"\")\n",
    "\n",
    "print(f'\\n\\nFSCORE:\\t{fscore}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
