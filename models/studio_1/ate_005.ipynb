{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nearby-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "healthy-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fourth-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "greater-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_org = pd.read_json(path+'ATE_ABSITA_training_set/ate_absita_training.ndjson', lines=True)\n",
    "\n",
    "train_sin = pd.read_csv(path+'ATE_ABSITA_training_set/sinonimi.csv')\n",
    "\n",
    "dev = pd.concat([pd.read_json(path+'ATE_ABSITA_dev_set/ate_absita_dev.ndjson', lines=True),\n",
    "                 pd.read_csv(path+'ATE_ABSITA_dev_set/sinonimi.csv')],\n",
    "                ignore_index=True)\n",
    "\n",
    "test_org = pd.read_json(path+'ATE_ABSITA_test_set/ate_absita_gold.ndjson', lines=True)\n",
    "\n",
    "test_sin = pd.read_csv(path+'ATE_ABSITA_test_set/sinonimi.csv')\n",
    "\n",
    "\n",
    "data_sinonimi = pd.read_csv(path+\"ATE_ABSITA_test_set/sinonimi.csv\")\n",
    "data_embedding = pd.read_csv(path+\"ATE_ABSITA_test_set/embedding.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "competent-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_study_1 = pd.concat([train_org, train_sin], ignore_index=True).sample(frac=1)\n",
    "test_study_1 = pd.concat([test_org, test_sin], ignore_index=True).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "surprising-reservoir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contains 6108 sentences\n",
      "Contains 109 sentences\n",
      "Contains 2400 sentences\n",
      "Contains 1200 sentences\n",
      "Contains 1200 sentences\n"
     ]
    }
   ],
   "source": [
    "train_study_1.drop(columns=['id_sentence','polarities','aspects_position','aspects'], inplace=True)\n",
    "dev.drop(columns=['id_sentence','polarities','aspects_position','aspects'], inplace=True)\n",
    "test_org.drop(columns=['id_sentence','polarities','aspects_position','aspects'], inplace=True)\n",
    "test_study_1.drop(columns=['id_sentence','polarities','aspects_position','aspects'], inplace=True)\n",
    "data_sinonimi.drop(columns=['polarities','aspects_position','aspects'], inplace=True)\n",
    "data_embedding.drop(columns=['polarities','aspects_position','aspects'], inplace=True)\n",
    "\n",
    "print(f'Contains {len(train_study_1)} sentences')\n",
    "print(f'Contains {len(dev)} sentences')\n",
    "print(f'Contains {len(test_study_1)} sentences')\n",
    "print(f'Contains {len(data_sinonimi)} sentences')\n",
    "print(f'Contains {len(data_embedding)} sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "according-revelation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN::\n",
      "pos    4300\n",
      "neg    1808\n",
      "Name: review_type, dtype: int64\n",
      "DEV::\n",
      "pos    86\n",
      "neg    23\n",
      "Name: review_type, dtype: int64\n",
      "TEST::\n",
      "pos    1714\n",
      "neg     686\n",
      "Name: review_type, dtype: int64\n",
      "SINONIMI::\n",
      "pos    857\n",
      "neg    343\n",
      "Name: review_type, dtype: int64\n",
      "EMBEDDING::\n",
      "pos    857\n",
      "neg    343\n",
      "Name: review_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_study_1[\"review_type\"] = train_study_1[\"score\"].apply(lambda x: \"neg\" if x < 5 else \"pos\")\n",
    "dev[\"review_type\"] = dev[\"score\"].apply(lambda x: \"neg\" if x < 5 else \"pos\")\n",
    "test_org[\"review_type\"] = test_org[\"score\"].apply(lambda x: \"neg\" if x < 5 else \"pos\")\n",
    "test_study_1[\"review_type\"] = test_study_1[\"score\"].apply(lambda x: \"neg\" if x < 5 else \"pos\")\n",
    "data_sinonimi[\"review_type\"] = data_sinonimi[\"score\"].apply(lambda x: \"neg\" if x < 5 else \"pos\")\n",
    "data_embedding[\"review_type\"] = data_embedding[\"score\"].apply(lambda x: \"neg\" if x < 5 else \"pos\")\n",
    "\n",
    "print(f'TRAIN::\\n{train_study_1.review_type.value_counts()}')\n",
    "print(f'DEV::\\n{dev.review_type.value_counts()}')\n",
    "print(f'TEST::\\n{test_study_1.review_type.value_counts()}')\n",
    "print(f'SINONIMI::\\n{data_sinonimi.review_type.value_counts()}')\n",
    "print(f'EMBEDDING::\\n{data_embedding.review_type.value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "parental-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_study_1.drop(columns=['score'], inplace=True)\n",
    "dev.drop(columns=['score'], inplace=True)\n",
    "test_org.drop(columns=['score'], inplace=True)\n",
    "test_study_1.drop(columns=['score'], inplace=True)\n",
    "data_sinonimi.drop(columns=['score'], inplace=True)\n",
    "data_embedding.drop(columns=['score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "precious-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_text_to_word_sequence(sentence):\n",
    "    return keras.preprocessing.text.text_to_word_sequence(sentence,\n",
    "                                                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`\\'{|}~\\t\\n',\n",
    "                                                          lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-washington",
   "metadata": {},
   "source": [
    "# OneHotEncode delle frasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "light-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_train = OneHotEncoder(sparse=False).fit_transform(\n",
    "        train_study_1.review_type.to_numpy().reshape(-1, 1))\n",
    "\n",
    "sentences = [my_text_to_word_sequence(sentence) for sentence in train_study_1['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fewer-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_dev = OneHotEncoder(sparse=False).fit_transform(\n",
    "        dev.review_type.to_numpy().reshape(-1, 1))\n",
    "\n",
    "sentences_dev = [my_text_to_word_sequence(sentence) for sentence in dev['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "differential-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_test_org = OneHotEncoder(sparse=False).fit_transform(\n",
    "        test_org.review_type.to_numpy().reshape(-1, 1))\n",
    "\n",
    "sentences_test_org = [my_text_to_word_sequence(sentence) for sentence in test_org['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pretty-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_test = OneHotEncoder(sparse=False).fit_transform(\n",
    "        test_study_1.review_type.to_numpy().reshape(-1, 1))\n",
    "\n",
    "sentences_test = [my_text_to_word_sequence(sentence) for sentence in test_study_1['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "activated-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_sin = OneHotEncoder(sparse=False).fit_transform(\n",
    "        data_sinonimi.review_type.to_numpy().reshape(-1, 1))\n",
    "\n",
    "sentences_sin = [my_text_to_word_sequence(sentence) for sentence in data_sinonimi['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "radical-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_emb = OneHotEncoder(sparse=False).fit_transform(\n",
    "        data_embedding.review_type.to_numpy().reshape(-1, 1))\n",
    "\n",
    "sentences_emb = [my_text_to_word_sequence(sentence) for sentence in data_embedding['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "illegal-phone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il massimo è 85\n"
     ]
    }
   ],
   "source": [
    "max_index, max = (-1, -1)\n",
    "for i, sentence in enumerate(sentences):\n",
    "  max_index, max = (i, len(sentence)) if len(sentence) > max else (max_index, max)\n",
    "for i, sentence in enumerate(sentences_dev):\n",
    "  max_index, max = (i, len(sentence)) if len(sentence) > max else (max_index, max)\n",
    "for i, sentence in enumerate(sentences_test):\n",
    "  max_index, max = (i, len(sentence)) if len(sentence) > max else (max_index, max)\n",
    "for i, sentence in enumerate(sentences_sin):\n",
    "  max_index, max = (i, len(sentence)) if len(sentence) > max else (max_index, max)\n",
    "for i, sentence in enumerate(sentences_emb):\n",
    "  max_index, max = (i, len(sentence)) if len(sentence) > max else (max_index, max)\n",
    "\n",
    "\n",
    "print(f'Il massimo è {max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-convertible",
   "metadata": {},
   "source": [
    "# Embedding delle frasi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "antique-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"word2index.pkl\", 'rb') as output:\n",
    "  w2i = pickle.load(output)\n",
    "with open(path+\"embedding_matrix.pkl\", 'rb') as output:\n",
    "  embedding_matrix = pickle.load(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "noted-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_trainset = np.zeros(shape=(len(sentences), max, 300))\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for j, word in enumerate(sentence):\n",
    "        try:\n",
    "            trainset[i, j, :] = embedding_matrix[w2i[word]]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "invisible-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_devset = np.zeros(shape=(len(sentences_dev), max, 300))\n",
    "for i, sentence in enumerate(sentences_dev):\n",
    "    for j, word in enumerate(sentence):\n",
    "        try:\n",
    "            embedded_devset[i, j, :] = embedding_matrix[w2i[word]]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "completed-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_testset_org = np.zeros(shape=(len(sentences_test_org), max, 300))\n",
    "for i, sentence in enumerate(sentences_test_org):\n",
    "    for j, word in enumerate(sentence):\n",
    "        try:\n",
    "            embedded_testset_org[i, j, :] = embedding_matrix[w2i[word]]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "technological-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_testset = np.zeros(shape=(len(sentences_test), max, 300))\n",
    "for i, sentence in enumerate(sentences_test):\n",
    "    for j, word in enumerate(sentence):\n",
    "        try:\n",
    "            embedded_testset[i, j, :] = embedding_matrix[w2i[word]]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "everyday-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_sin = np.zeros(shape=(len(sentences_sin), max, 300))\n",
    "for i, sentence in enumerate(sentences_sin):\n",
    "    for j, word in enumerate(sentence):\n",
    "        try:\n",
    "            embedded_sin[i, j, :] = embedding_matrix[w2i[word]]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "advance-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_emb = np.zeros(shape=(len(sentences_emb), max, 300))\n",
    "for i, sentence in enumerate(sentences_emb):\n",
    "    for j, word in enumerate(sentence):\n",
    "        try:\n",
    "            embedded_emb[i, j, :] = embedding_matrix[w2i[word]]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-neutral",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "exempt-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = optuna.load_study(study_name=\"ATE\", storage=\"sqlite:///\"+path+\"optuna_ATE_studio_0.db\").best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "crude-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Input(shape=(max, 300)))\n",
    "model.add(keras.layers.Bidirectional(layer=keras.layers.LSTM(units=best_params[\"units\"],\n",
    "                                                             recurrent_dropout=best_params[\"dropout\"],\n",
    "                                                             activation='tanh')))\n",
    "model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=keras.optimizers.Adam(0.001),\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "material-slave",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "112/112 [==============================] - 14s 126ms/step - loss: 0.4492 - accuracy: 0.7947 - val_loss: 0.4061 - val_accuracy: 0.8349\n",
      "Epoch 2/100\n",
      "112/112 [==============================] - 14s 126ms/step - loss: 0.4257 - accuracy: 0.8098 - val_loss: 0.4378 - val_accuracy: 0.7752\n",
      "Epoch 3/100\n",
      "112/112 [==============================] - 14s 126ms/step - loss: 0.3993 - accuracy: 0.8175 - val_loss: 0.5199 - val_accuracy: 0.7752\n",
      "Epoch 4/100\n",
      "112/112 [==============================] - 14s 126ms/step - loss: 0.3668 - accuracy: 0.8400 - val_loss: 0.4333 - val_accuracy: 0.7936\n",
      "Epoch 5/100\n",
      "112/112 [==============================] - 14s 126ms/step - loss: 0.3395 - accuracy: 0.8535 - val_loss: 0.4915 - val_accuracy: 0.8211\n",
      "Epoch 6/100\n",
      "112/112 [==============================] - 14s 126ms/step - loss: 0.3069 - accuracy: 0.8639 - val_loss: 0.8108 - val_accuracy: 0.6697\n",
      "Epoch 7/100\n",
      "112/112 [==============================] - 14s 125ms/step - loss: 0.3059 - accuracy: 0.8687 - val_loss: 0.7450 - val_accuracy: 0.7156\n",
      "Epoch 8/100\n",
      "112/112 [==============================] - 14s 125ms/step - loss: 0.2966 - accuracy: 0.8733 - val_loss: 0.6081 - val_accuracy: 0.7798\n",
      "Epoch 9/100\n",
      "112/112 [==============================] - 14s 126ms/step - loss: 0.2298 - accuracy: 0.9055 - val_loss: 0.5404 - val_accuracy: 0.7936\n",
      "Epoch 10/100\n",
      "112/112 [==============================] - 14s 127ms/step - loss: 0.2112 - accuracy: 0.9149 - val_loss: 0.6350 - val_accuracy: 0.7844\n",
      "Epoch 11/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.1911 - accuracy: 0.9206 - val_loss: 0.5840 - val_accuracy: 0.7752\n"
     ]
    }
   ],
   "source": [
    "result = model.fit(embedded_trainset,\n",
    "                   one_hot_train,\n",
    "                   validation_data=(embedded_devset, one_hot_dev),\n",
    "                   epochs=100,\n",
    "                   batch_size=best_params[\"batch_size\"],\n",
    "                   callbacks=[keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                                            patience=10,\n",
    "                                                            restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "boring-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('ATE_w_studio1_005.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-baghdad",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "available-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-celtic",
   "metadata": {},
   "source": [
    "## DATASET ORIGINARIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dangerous-firmware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 3s 27ms/step - loss: 0.4303 - accuracy: 0.7993\n",
      "DATASET ORIGINARIO[0.4303032159805298, 0.7992796301841736]\n"
     ]
    }
   ],
   "source": [
    "result_base = model.evaluate(embedded_trainset,one_hot_train,batch_size=best_params['batch_size'])\n",
    "print(f'DATASET ORIGINARIO{result_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "clear-noise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\tP\tN\n",
      "\n",
      "P\t0\t1808\n",
      "\n",
      "N\t0\t4300\n",
      "\n",
      "\n",
      "FSCORE:\t0.8262874711760183\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(embedded_trainset, batch_size=best_params['batch_size'])\n",
    "cm = confusion_matrix(one_hot_train.argmax(axis=1), pred.argmax(axis=1))\n",
    "fscore = f1_score(one_hot_train.argmax(axis=1), pred.argmax(axis=1))\n",
    "\n",
    "print(f\"\"\"Confusion Matrix:\n",
    "\\tP\\tN\\n\n",
    "P\\t{cm[0][0]}\\t{cm[0][1]}\\n\n",
    "N\\t{cm[1][0]}\\t{cm[1][1]}\"\"\")\n",
    "\n",
    "print(f'\\n\\nFSCORE:\\t{fscore}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-croatia",
   "metadata": {},
   "source": [
    "## TESTSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "recovered-edgar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 1s 27ms/step - loss: 0.5101 - accuracy: 0.7696\n",
      "DATASET TEST[0.510076642036438, 0.7695833444595337]\n"
     ]
    }
   ],
   "source": [
    "result_base=model.evaluate(embedded_testset,one_hot_test,batch_size=best_params['batch_size'])\n",
    "print(f'DATASET TEST{result_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "turkish-update",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\tP\tN\n",
      "\n",
      "P\t232\t454\n",
      "\n",
      "N\t99\t1615\n",
      "\n",
      "\n",
      "FSCORE:\t0.8538197197991012\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(embedded_testset, batch_size=best_params['batch_size'])\n",
    "cm = confusion_matrix(one_hot_test.argmax(axis=1), pred.argmax(axis=1))\n",
    "fscore = f1_score(one_hot_test.argmax(axis=1), pred.argmax(axis=1))\n",
    "\n",
    "print(f\"\"\"Confusion Matrix:\n",
    "\\tP\\tN\\n\n",
    "P\\t{cm[0][0]}\\t{cm[0][1]}\\n\n",
    "N\\t{cm[1][0]}\\t{cm[1][1]}\"\"\")\n",
    "\n",
    "print(f'\\n\\nFSCORE:\\t{fscore}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-initial",
   "metadata": {},
   "source": [
    "## TESTSET ORIGINARIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "resistant-century",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 1s 28ms/step - loss: 0.5102 - accuracy: 0.7700\n",
      "DATASET TEST[0.5101757049560547, 0.7699999809265137]\n"
     ]
    }
   ],
   "source": [
    "result_base=model.evaluate(embedded_testset_org,one_hot_test_org,batch_size=best_params['batch_size'])\n",
    "print(f'DATASET TEST{result_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "convertible-diameter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\tP\tN\n",
      "\n",
      "P\t117\t226\n",
      "\n",
      "N\t50\t807\n",
      "\n",
      "\n",
      "FSCORE:\t0.8539682539682539\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(embedded_testset_org, batch_size=best_params['batch_size'])\n",
    "cm = confusion_matrix(one_hot_test_org.argmax(axis=1), pred.argmax(axis=1))\n",
    "fscore = f1_score(one_hot_test_org.argmax(axis=1), pred.argmax(axis=1))\n",
    "\n",
    "print(f\"\"\"Confusion Matrix:\n",
    "\\tP\\tN\\n\n",
    "P\\t{cm[0][0]}\\t{cm[0][1]}\\n\n",
    "N\\t{cm[1][0]}\\t{cm[1][1]}\"\"\")\n",
    "\n",
    "print(f'\\n\\nFSCORE:\\t{fscore}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-template",
   "metadata": {},
   "source": [
    "## DATASET SINONIMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "established-ideal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 1s 28ms/step - loss: 0.5100 - accuracy: 0.7692\n",
      "DATASET SINONIMI[0.509977400302887, 0.7691666483879089]\n"
     ]
    }
   ],
   "source": [
    "result_base=model.evaluate(embedded_sin,one_hot_sin,batch_size=best_params['batch_size'])\n",
    "print(f'DATASET SINONIMI{result_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "female-lounge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\tP\tN\n",
      "\n",
      "P\t115\t228\n",
      "\n",
      "N\t49\t808\n",
      "\n",
      "\n",
      "FSCORE:\t0.8536714210248284\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(embedded_sin, batch_size=best_params['batch_size'])\n",
    "cm = confusion_matrix(one_hot_sin.argmax(axis=1), pred.argmax(axis=1))\n",
    "fscore = f1_score(one_hot_sin.argmax(axis=1), pred.argmax(axis=1))\n",
    "\n",
    "print(f\"\"\"Confusion Matrix:\n",
    "\\tP\\tN\\n\n",
    "P\\t{cm[0][0]}\\t{cm[0][1]}\\n\n",
    "N\\t{cm[1][0]}\\t{cm[1][1]}\"\"\")\n",
    "\n",
    "print(f'\\n\\nFSCORE:\\t{fscore}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-spider",
   "metadata": {},
   "source": [
    "## DATASET EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dynamic-valve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 1s 27ms/step - loss: 0.5105 - accuracy: 0.7683\n",
      "DATASET EMBEDDING[0.510522723197937, 0.7683333158493042]\n"
     ]
    }
   ],
   "source": [
    "result_base=model.evaluate(embedded_emb,one_hot_emb,batch_size=best_params['batch_size'])\n",
    "print(f'DATASET EMBEDDING{result_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "toxic-husband",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\tP\tN\n",
      "\n",
      "P\t119\t224\n",
      "\n",
      "N\t54\t803\n",
      "\n",
      "\n",
      "FSCORE:\t0.8524416135881103\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(embedded_emb, batch_size=best_params['batch_size'])\n",
    "cm = confusion_matrix(one_hot_emb.argmax(axis=1), pred.argmax(axis=1))\n",
    "fscore = f1_score(one_hot_emb.argmax(axis=1), pred.argmax(axis=1))\n",
    "\n",
    "print(f\"\"\"Confusion Matrix:\n",
    "\\tP\\tN\\n\n",
    "P\\t{cm[0][0]}\\t{cm[0][1]}\\n\n",
    "N\\t{cm[1][0]}\\t{cm[1][1]}\"\"\")\n",
    "\n",
    "print(f'\\n\\nFSCORE:\\t{fscore}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
