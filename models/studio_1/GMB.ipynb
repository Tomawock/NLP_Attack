{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GMB.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tomawock/NLP_Attack/blob/main/models/studio_1/GMB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POK6j5oiTzzi"
      },
      "source": [
        "Parte nuova testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4D6g8HCVCnq"
      },
      "source": [
        "!pip install h5py\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLcY-sL2EW4T"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EsqztNaYcQE"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from sklearn import preprocessing\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCjC_sLPVsC7"
      },
      "source": [
        "data_original = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_dataset.tsv\"\n",
        "max_len = 384"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rsWWOi_YPtO"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False, reduction=tf.keras.losses.Reduction.SUM\n",
        ")\n",
        "\n",
        "# Ignoring loss that is calculated due to padded targets\n",
        "def masked_ce_loss(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 17))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "def create_model(num_tags):\n",
        "    # BERT Base model\n",
        "    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # NER Model\n",
        "\n",
        "    # Instantiate Keras tensors\n",
        "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    # TFBertmodel\n",
        "    embedding = encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
        "    # Add drop out layer\n",
        "    embedding = layers.Dropout(0.5)(embedding)\n",
        "    # Add softmax layer for classifying\n",
        "    tag_logits = layers.Dense(num_tags+1, activation='softmax')(embedding)\n",
        "    \n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask],\n",
        "        outputs=[tag_logits],\n",
        "    )\n",
        "    optimizer = keras.optimizers.Adam(lr=3e-5)\n",
        "    model.compile(optimizer=optimizer, loss=masked_ce_loss, metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl2kaHNtcO-F"
      },
      "source": [
        "#Reference - https://keras.io/examples/nlp/text_extraction_with_bert/\n",
        "# Save the slow pretrained tokenizer\n",
        "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "save_path = \"bert_base_uncased/\"\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "slow_tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Load the fast tokenizer from saved file\n",
        "tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBO2axNIFmXH"
      },
      "source": [
        "MUST USE GPU or model dont load "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQxXGab4Vg5U"
      },
      "source": [
        "parz = pd.read_csv(data_original,sep='\\t', encoding=\"latin-1\", index_col=[0])\n",
        "\n",
        "num_tags = parz[\"Tag\"].nunique()\n",
        "\n",
        "new_model = create_model(num_tags)\n",
        "    \n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mssGj3zjI2hh"
      },
      "source": [
        "###Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKYEp_xsaQJ4"
      },
      "source": [
        "# Get the sentences\n",
        "def process_csv(data_path):\n",
        "    df = pd.read_csv(data_path, encoding=\"latin-1\", na_filter = False)\n",
        "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
        "    enc_tag = preprocessing.LabelEncoder()\n",
        "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
        "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "    return sentences, tag, enc_tag\n",
        "\n",
        "# Get the sentences\n",
        "def process_tsv(data_path):\n",
        "    df =  pd.read_csv(data_path,sep='\\t', encoding=\"latin-1\", index_col=[0])\n",
        "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
        "    enc_tag = preprocessing.LabelEncoder()\n",
        "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
        "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "    return sentences, tag, enc_tag\n",
        "\n",
        "# Prepare the inputs for feeding into model\n",
        "def create_inputs_targets(data):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"tags\": []\n",
        "    }\n",
        "    _,ext=os.path.splitext(data)\n",
        "    if (ext== '.tsv'):\n",
        "      sentences, tags, tag_encoder = process_tsv(data)\n",
        "    else:\n",
        "      sentences, tags, tag_encoder = process_csv(data)\n",
        "    \n",
        "    for sentence, tag in zip(sentences, tags):\n",
        "        \n",
        "        input_ids = []\n",
        "        target_tags = []\n",
        "        for idx, word in enumerate(sentence):\n",
        "            ids = tokenizer.encode(word, add_special_tokens=False)\n",
        "            input_ids.extend(ids.ids)\n",
        "            num_tokens = len(ids)\n",
        "            target_tags.extend([tag[idx]] * num_tokens)\n",
        "                    \n",
        "        \n",
        "        # Pad truncate\n",
        "        input_ids = input_ids[:max_len - 2]\n",
        "        target_tags = target_tags[:max_len - 2]\n",
        "\n",
        "        # Add [CLS] and [SEP]\n",
        "        input_ids = [101] + input_ids + [102]\n",
        "        target_tags = [16] + target_tags + [16]\n",
        "        # token_type_ids does not matter as the task has only one sentence\n",
        "        token_type_ids = [0] * len(input_ids)\n",
        "        # Adding attention mask for non-padded input\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        \n",
        "        # Making all the input enbedding same lenting by padding\n",
        "        padding_len = max_len - len(input_ids)\n",
        "        input_ids = input_ids + ([0] * padding_len)\n",
        "        attention_mask = attention_mask + ([0] * padding_len)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "        target_tags = target_tags + ([17] * padding_len)\n",
        "        \n",
        "        dataset_dict[\"input_ids\"].append(input_ids)\n",
        "        dataset_dict[\"token_type_ids\"].append(token_type_ids)\n",
        "        dataset_dict[\"attention_mask\"].append(attention_mask)\n",
        "        dataset_dict[\"tags\"].append(target_tags)\n",
        "        assert len(target_tags) == max_len, f'{len(input_ids)}, {len(target_tags)}'\n",
        "                \n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    # Creating array of input embeddings\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"token_type_ids\"],\n",
        "        dataset_dict[\"attention_mask\"],\n",
        "    ]\n",
        "    y = dataset_dict[\"tags\"]\n",
        "    return x, y, tag_encoder"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y36IkdBRPaev"
      },
      "source": [
        "data_original_sinonimi_train = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_original_sinonimi_train.csv\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGeGXmfcI1jU",
        "outputId": "48ddf0bd-3592-47eb-ae1f-28581377415d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x_train, y_train, tag_encoder = create_inputs_targets(data_original_sinonimi_train)\n",
        "\n",
        "bs = 16\n",
        "\n",
        "new_model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=1,\n",
        "    verbose=1,\n",
        "    batch_size=bs,\n",
        "    validation_split=0.1\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "237/237 [==============================] - ETA: 0s - loss: 108.9126 - accuracy: 0.9407WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "237/237 [==============================] - 349s 1s/step - loss: 108.6115 - accuracy: 0.9408 - val_loss: 12.7377 - val_accuracy: 0.9921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcb752ca510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF-oLRMgV65C"
      },
      "source": [
        "new_model.save_weights(\"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_w_studio_1_model.h5\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u1o8pPySLR2"
      },
      "source": [
        "###Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-J7Xft_VWVT"
      },
      "source": [
        "new_model.load_weights(\"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_w_studio_1_model.h5\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkQHJBaBdfFU"
      },
      "source": [
        "datapath_original_sinonimi_test = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_original_sinonimi_test.csv\"\n",
        "datapath_original_test = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_original_test.csv\"\n",
        "datapath_sinonimi_test = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_sinonimi_test.csv\"\n",
        "datapath_embedding_test = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_embedding_test.csv\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds4YMx3teKZD"
      },
      "source": [
        "Original + sinonimi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72Mk0Zn4SkOX",
        "outputId": "57c7650a-71d9-4060-b9fd-1de471576e04"
      },
      "source": [
        "#crea i dati per essere elborati dal modello\n",
        "x_test, y_test, tag_encoder = create_inputs_targets(datapath_original_sinonimi_test)\n",
        "pred_test = new_model.evaluate(x_test,y_test,batch_size=16)\n",
        "#definisce il dizioanrio dell'associazione classi valore numerico"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "113/113 [==============================] - 54s 474ms/step - loss: 230.3171 - accuracy: 0.9350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cQdMq8Re3Qb"
      },
      "source": [
        "Original\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFc4tfOHe5RE",
        "outputId": "2f48fd00-02f4-4a90-8cc7-6681ff4673a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#crea i dati per essere elborati dal modello\n",
        "x_test, y_test, tag_encoder = create_inputs_targets(datapath_original_test)\n",
        "pred_test = new_model.evaluate(x_test,y_test,batch_size=16)\n",
        "#definisce il dizioanrio dell'associazione classi valore numerico"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 26s 448ms/step - loss: 201.6224 - accuracy: 0.9408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-DCDEnySdno"
      },
      "source": [
        "Sinonimi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVfSNN8vV6-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "763f59bc-acbb-4001-a3ff-fd2ebffe8a0e"
      },
      "source": [
        "#crea i dati per essere elborati dal modello\n",
        "x_test, y_test, tag_encoder = create_inputs_targets(datapath_sinonimi_test)\n",
        "pred_test = new_model.evaluate(x_test,y_test,batch_size=16)\n",
        "#definisce il dizioanrio dell'associazione classi valore numerico"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 26s 462ms/step - loss: 259.1384 - accuracy: 0.9292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4HTD1kTSf89"
      },
      "source": [
        "Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT_yp3H2QTpf",
        "outputId": "2651904e-4d60-4431-9ff6-d9d29d899f15"
      },
      "source": [
        "#crea i dati per essere elborati dal modello\n",
        "x_test, y_test, tag_encoder = create_inputs_targets(datapath_embedding_test)\n",
        "pred_test = new_model.evaluate(x_test,y_test,batch_size=16)\n",
        "#definisce il dizioanrio dell'associazione classi valore numerico"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 26s 456ms/step - loss: 244.1104 - accuracy: 0.9327\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}