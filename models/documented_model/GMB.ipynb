{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GMB.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tomawock/NLP_Attack/blob/main/models/documented_model/GMB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWEjvJpXb6vB"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-tBaZCGBoBe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ieT6zAORaLZ"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn import preprocessing\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
        "import datetime\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import os\n",
        "print(tf.__version__)\n",
        "\n",
        "transformers.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rGqIDfvdFOg"
      },
      "source": [
        "# Get the sentences\n",
        "def process_csv(data_path):\n",
        "    df = pd.read_csv(data,sep='\\t', encoding=\"latin-1\", index_col=[0])\n",
        "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
        "    enc_tag = preprocessing.LabelEncoder()\n",
        "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
        "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "    return sentences, tag, enc_tag\n",
        "\n",
        "# Prepare the inputs for feeding into model\n",
        "def create_inputs_targets(data):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"tags\": []\n",
        "    }\n",
        "    sentences, tags, tag_encoder = process_csv(data)\n",
        "    print(sentences[0])\n",
        "    \n",
        "    for sentence, tag in zip(sentences, tags):\n",
        "        \n",
        "        input_ids = []\n",
        "        target_tags = []\n",
        "        for idx, word in enumerate(sentence):\n",
        "            ids = tokenizer.encode(word, add_special_tokens=False)\n",
        "            input_ids.extend(ids.ids)\n",
        "            num_tokens = len(ids)\n",
        "            target_tags.extend([tag[idx]] * num_tokens)\n",
        "                    \n",
        "        \n",
        "        # Pad truncate\n",
        "        input_ids = input_ids[:max_len - 2]\n",
        "        target_tags = target_tags[:max_len - 2]\n",
        "\n",
        "        # Add [CLS] and [SEP]\n",
        "        input_ids = [101] + input_ids + [102]\n",
        "        target_tags = [16] + target_tags + [16]\n",
        "        # token_type_ids does not matter as the task has only one sentence\n",
        "        token_type_ids = [0] * len(input_ids)\n",
        "        # Adding attention mask for non-padded input\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        \n",
        "        # Making all the input enbedding same lenting by padding\n",
        "        padding_len = max_len - len(input_ids)\n",
        "        input_ids = input_ids + ([0] * padding_len)\n",
        "        attention_mask = attention_mask + ([0] * padding_len)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "        target_tags = target_tags + ([17] * padding_len)\n",
        "        \n",
        "        dataset_dict[\"input_ids\"].append(input_ids)\n",
        "        dataset_dict[\"token_type_ids\"].append(token_type_ids)\n",
        "        dataset_dict[\"attention_mask\"].append(attention_mask)\n",
        "        dataset_dict[\"tags\"].append(target_tags)\n",
        "        assert len(target_tags) == max_len, f'{len(input_ids)}, {len(target_tags)}'\n",
        "                \n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    # Creating array of input embeddings\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"token_type_ids\"],\n",
        "        dataset_dict[\"attention_mask\"],\n",
        "    ]\n",
        "    y = dataset_dict[\"tags\"]\n",
        "    return x, y, tag_encoder"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx_nPK0lcHy6"
      },
      "source": [
        "#Read data path\n",
        "data = \"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_dataset.tsv\"\n",
        "max_len = 384\n",
        "configuration = BertConfig()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl2kaHNtcO-F"
      },
      "source": [
        "#Reference - https://keras.io/examples/nlp/text_extraction_with_bert/\n",
        "# Save the slow pretrained tokenizer\n",
        "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "save_path = \"bert_base_uncased/\"\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "slow_tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Load the fast tokenizer from saved file\n",
        "tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOgvB-WUdUgq"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False, reduction=tf.keras.losses.Reduction.SUM\n",
        ")\n",
        "\n",
        "# Ignoring loss that is calculated due to padded targets\n",
        "def masked_ce_loss(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 17))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "def create_model(num_tags):\n",
        "    # BERT Base model\n",
        "    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # NER Model\n",
        "\n",
        "    # Instantiate Keras tensors\n",
        "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    # TFBertmodel\n",
        "    embedding = encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
        "    # Add drop out layer\n",
        "    embedding = layers.Dropout(0.5)(embedding)\n",
        "    # Add softmax layer for classifying\n",
        "    tag_logits = layers.Dense(num_tags+1, activation='softmax')(embedding)\n",
        "    \n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask],\n",
        "        outputs=[tag_logits],\n",
        "    )\n",
        "    optimizer = keras.optimizers.Adam(lr=3e-5)\n",
        "    model.compile(optimizer=optimizer, loss=masked_ce_loss, metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FE-iQeIdcMA"
      },
      "source": [
        "parz = pd.read_csv(data,sep='\\t', encoding=\"latin-1\", index_col=[0])\n",
        "\n",
        "num_tags = parz[\"Tag\"].nunique()\n",
        "\n",
        "use_tpu = None\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    use_tpu = True\n",
        "except:\n",
        "    use_tpu = False\n",
        "\n",
        "if use_tpu:\n",
        "    # Create distribution strategy\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "    # Create model\n",
        "    with strategy.scope():\n",
        "        model = create_model(num_tags)\n",
        "else:\n",
        "    model = create_model(num_tags)\n",
        "    \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W09Zw_GdRdV"
      },
      "source": [
        "x_train, y_train, tag_encoder = create_inputs_targets(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vt7lIUAfKbM"
      },
      "source": [
        "x_train[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bC0GPFCRvv-"
      },
      "source": [
        "Dont use TPU or result are bad sice BS is too big and the result are much worse "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09jEXcEHdhqH"
      },
      "source": [
        "x_train, y_train, tag_encoder = create_inputs_targets(data)\n",
        "\n",
        "bs = 64 if use_tpu else 16\n",
        "\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=1,\n",
        "    verbose=1,\n",
        "    batch_size=bs,\n",
        "    validation_split=0.1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7FgelqYdn13"
      },
      "source": [
        "def create_inputs_targets_from_text(data):  \n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"tags\": []\n",
        "    }\n",
        "    n_tokens=[]\n",
        "    for sen in data:\n",
        "        print(sen)\n",
        "        sentence = sen.split(\" \")\n",
        "        input_ids = []\n",
        "        \n",
        "        #target_tags = []\n",
        "        print(sentence)\n",
        "        for idx, word in enumerate(sentence):\n",
        "            ids = tokenizer.encode(word, add_special_tokens=False)\n",
        "            input_ids.extend(ids.ids)\n",
        "            num_tokens = len(ids)\n",
        "            \n",
        "        # Pad truncate\n",
        "        input_ids = input_ids[:max_len - 2]\n",
        "\n",
        "        # Add [CLS] and [SEP]\n",
        "        input_ids = [101] + input_ids + [102]\n",
        "        n_tokens.append(len(input_ids)) \n",
        "        # token_type_ids does not matter as the task has only one sentence\n",
        "        token_type_ids = [0] * len(input_ids)\n",
        "        # Adding attention mask for non-padded input\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        \n",
        "        # Making all the input enbedding same lenting by padding\n",
        "        padding_len = max_len - len(input_ids)\n",
        "        input_ids = input_ids + ([0] * padding_len)\n",
        "        attention_mask = attention_mask + ([0] * padding_len)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "        #target_tags = target_tags + ([17] * padding_len)\n",
        "        \n",
        "        dataset_dict[\"input_ids\"].append(input_ids)\n",
        "        dataset_dict[\"token_type_ids\"].append(token_type_ids)\n",
        "        dataset_dict[\"attention_mask\"].append(attention_mask)\n",
        "        #assert len(target_tags) == max_len, f'{len(input_ids)}, {len(target_tags)}'\n",
        "                \n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    # Creating array of input embeddings\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"token_type_ids\"],\n",
        "        dataset_dict[\"attention_mask\"],\n",
        "    ]\n",
        "    #y = dataset_dict[\"tags\"]\n",
        "    return x, dataset_dict, n_tokens"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYt7QKlZNGnJ"
      },
      "source": [
        "model.save_weights(\"/content/drive/Shareddrives/Deep Learning/datasets/GMB/GMB_w_using_GPU.h5\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yRXqHHanC1I"
      },
      "source": [
        "# test input sentence\n",
        "test_inputs = [\"John Kennedy was born in New York in august\",\"Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\"]\n",
        "#crea i dati per essere elborati dal modello\n",
        "x_test, y_test, n_tokens = create_inputs_targets_from_text(test_inputs)\n",
        "\n",
        "pred_test = model.predict(x_test)\n",
        "#definisce il dizioanrio dell'associazione classi valore numerico\n",
        "le_dict = dict(zip(tag_encoder.transform(tag_encoder.classes_), tag_encoder.classes_))\n",
        "result =[]\n",
        "for i in range(len(test_inputs)):\n",
        "  pred_tags = np.argmax(pred_test,2)[i][:n_tokens[i]]#trova il vettore delle classsi associato ad ongi stringa in ingresso\n",
        "  pred_tags = [le_dict.get(_, '[pad]') for _ in pred_tags]\n",
        "  result.append(pred_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63ryTmk5dt5i",
        "outputId": "67e61381-352a-4c30-d8e2-badb201ec865"
      },
      "source": [
        "print(result[0])\n",
        "print(result[1])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['O', 'I-per', 'I-per', 'O', 'O', 'O', 'B-geo', 'B-geo', 'O', 'B-tim', 'O']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}