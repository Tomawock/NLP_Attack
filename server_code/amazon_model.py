# -*- coding: utf-8 -*-
"""Amazon_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Tomawock/NLP_Attack/blob/main/model/Amazon_Model.ipynb
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install tensorflow_text

# nostri import 
import random
import timeit
import pickle


import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import seaborn as sns
from pylab import rcParams
from tqdm import tqdm
import matplotlib.pyplot as plt
from matplotlib import rc
from pandas.plotting import register_matplotlib_converters
from sklearn.model_selection import train_test_split
import tensorflow_hub as hub
import tensorflow_text
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from sklearn.preprocessing import OneHotEncoder

tf.test.is_gpu_available()

RANDOM_SEED = 42

np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

df = pd.read_csv('/content/drive/Shareddrives/Deep Learning/datasets/Amazon/Reviews.csv')
print(f"COLONNE:{df.columns}")
print(f"DIMENSIONE:{df.shape}")

del df['Id']
del df['ProductId']
del df['UserId']
del df['ProfileName']
del df['HelpfulnessNumerator']
del df['HelpfulnessDenominator']
del df['Time']

df['review'] = df['Summary']+df['Text']
del df['Summary']
del df['Text']
df.review.fillna("",inplace = True)
df.head()

df["review_type"] = df["Score"].apply(lambda x: "negative" if x < 4 else "positive")

df.review_type.value_counts()

positive_reviews = df[df.review_type == "positive"]
negative_reviews = df[df.review_type == "negative"]

positive_df = positive_reviews.sample(n=len(negative_reviews), random_state=RANDOM_SEED)
negative_df = negative_reviews

review_df = positive_df.append(negative_df).reset_index(drop=True)
review_df.shape

"""## **Modello unico**"""

type_one_hot = OneHotEncoder(sparse=False).fit_transform(
  review_df.review_type.to_numpy().reshape(-1, 1)
)

train_reviews, test_reviews, y_train, y_test =\
  train_test_split(
    review_df.review, 
    type_one_hot, 
    test_size=.1, 
    random_state=RANDOM_SEED
  )

train_reviews.shape

embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3")

def UniversalEmbedding(x):
    return embed(tf.squeeze(tf.cast(x, tf.string)))#, 
    	#signature="default", as_dict=True)["default"]

model = keras.Sequential()

model.add(keras.layers.Input(shape=(1,), dtype=tf.string))
model.add(keras.layers.Lambda(UniversalEmbedding,output_shape=(512,)))
model.add(keras.layers.Dense(units=256, activation='relu'))
model.add(keras.layers.Dropout(rate=0.2))
model.add(keras.layers.Dense(units=128, activation='relu'))
model.add(keras.layers.Dropout(rate=0.2))
model.add(keras.layers.Dense(2, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(0.001), metrics=['accuracy'])
model.summary()

history = model.fit(
    train_reviews[:2500], y_train[:2500], 
    epochs=15, 
    batch_size=16, 
    validation_split=0.1, 
    verbose=1, 
    shuffle=True
)

new_text = ["BAD", 
            "What is the highest peak in California ?", 
            "Who invented the light bulb ?"]

new_text = np.array(new_text, dtype=object)[:, np.newaxis]

model.predict(new_text[0])